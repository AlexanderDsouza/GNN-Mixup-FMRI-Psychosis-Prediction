{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwyNZ5yZoKHuhJq4MMEHL3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CJfB91QN5FXO","executionInfo":{"status":"ok","timestamp":1755797433098,"user_tz":420,"elapsed":4197,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"outputs":[],"source":["# Core libraries\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","from scipy.linalg import eigh\n","\n"]},{"cell_type":"code","source":["# ===========================\n","# Load and preprocess dataset\n","# ===========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","df = pd.read_excel(file_path)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uT2NKed5VEJ","executionInfo":{"status":"ok","timestamp":1755797482381,"user_tz":420,"elapsed":1994,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"b979f1e7-ffc2-4650-8b55-707b3bb46256"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#creating graphs\n","\n","\n","# Drop SID column and get all edge columns\n","edge_columns = df.columns.drop('SID')\n","\n","# Extract all unique node names (stripped of 'ABS_' if present)\n","nodes_set = set()\n","for col in edge_columns:\n","    raw_node1, raw_node2 = col.split('-')\n","    node1 = raw_node1.replace('ABS_', '')\n","    node2 = raw_node2.replace('ABS_', '')\n","    nodes_set.add(node1)\n","    nodes_set.add(node2)\n","\n","# Sort nodes for consistent ordering\n","nodes = sorted(list(nodes_set))\n","n_nodes = len(nodes)\n","\n","# Map node name to index\n","node_to_idx = {node: i for i, node in enumerate(nodes)}\n","\n","# Separate edges into ABS and non-ABS edges\n","abs_edge_cols = []\n","rel_edge_cols = []\n","abs_edge_to_idx = []\n","rel_edge_to_idx = []\n","\n","for col in edge_columns:\n","    raw_node1, raw_node2 = col.split('-')\n","    node1 = raw_node1.replace('ABS_', '')\n","    node2 = raw_node2.replace('ABS_', '')\n","    idx1, idx2 = node_to_idx[node1], node_to_idx[node2]\n","\n","    if 'ABS_' in col:\n","        abs_edge_cols.append(col)\n","        abs_edge_to_idx.append((idx1, idx2))\n","    else:\n","        rel_edge_cols.append(col)\n","        rel_edge_to_idx.append((idx1, idx2))\n","\n","# Build adjacency matrices for each row\n","abs_adj_matrices = []\n","rel_adj_matrices = []\n","\n","for _, row in df.iterrows():\n","    # Create zero matrices for ABS and relative graphs\n","    abs_adj = np.zeros((n_nodes, n_nodes))\n","    rel_adj = np.zeros((n_nodes, n_nodes))\n","\n","    # Fill ABS adjacency matrix\n","    for col_idx, (i1, i2) in enumerate(abs_edge_to_idx):\n","        val = row[abs_edge_cols[col_idx]]\n","        abs_adj[i1, i2] = val\n","        abs_adj[i2, i1] = val  # symmetric\n","\n","    # Fill relative adjacency matrix\n","    for col_idx, (i1, i2) in enumerate(rel_edge_to_idx):\n","        val = row[rel_edge_cols[col_idx]]\n","        rel_adj[i1, i2] = val\n","        rel_adj[i2, i1] = val  # symmetric\n","\n","    abs_adj_matrices.append(abs_adj)\n","    rel_adj_matrices.append(rel_adj)\n"],"metadata":{"id":"IYwH0Lru5XMd","executionInfo":{"status":"ok","timestamp":1755797483578,"user_tz":420,"elapsed":151,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#making a dictionary of sid to absolute graph and relative graph\n","\n","graphs_by_sid = {}\n","\n","for sid, abs_mat, rel_mat in zip(df['SID'], abs_adj_matrices, rel_adj_matrices):\n","    graphs_by_sid[sid] = {\n","        'abs': abs_mat,\n","        'rel': rel_mat\n","    }\n","\n","print(graphs_by_sid['epp348']['rel'])  # Relative graph\n","print(graphs_by_sid['epp348']['rel'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6RImj5D5cXD","executionInfo":{"status":"ok","timestamp":1755797485389,"user_tz":420,"elapsed":11,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"c43130ec-465a-4647-eac0-b92cca904634","collapsed":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.          0.6443512   0.60952988  0.48149378  0.21855582  0.02744795\n","   0.19705619 -0.14626492 -0.20716272 -0.01932991 -0.15652164 -0.28049982\n","  -0.17138803 -0.17873779 -0.05382542]\n"," [ 0.6443512   0.          0.53500292  0.64447851  0.71385829  0.0261051\n","   0.44132225 -0.55944315 -0.61724242 -0.10934934 -0.25394405 -0.15493399\n","  -0.18232735 -0.12858349 -0.11987415]\n"," [ 0.60952988  0.53500292  0.          0.50661799  0.37571928  0.42134627\n","   0.04587733 -0.12207047 -0.50220895 -0.05944264  0.08870163  0.1442684\n","  -0.41469532  0.02158323 -0.22569421]\n"," [ 0.48149378  0.64447851  0.50661799  0.          1.09910671  0.18484547\n","   0.21678365 -0.36616225 -0.53407179 -0.16535064 -0.03218564  0.05986651\n","  -0.11841703 -0.09234453 -0.26826495]\n"," [ 0.21855582  0.71385829  0.37571928  1.09910671  0.          0.10228209\n","   0.24679883 -0.45983477 -0.57815033 -0.17797306  0.07242114  0.08410655\n","  -0.13201032 -0.1261167  -0.20566083]\n"," [ 0.02744795  0.0261051   0.42134627  0.18484547  0.10228209  0.\n","   0.0339953   0.16103661 -0.08961375  0.00490449 -0.14571103  0.13506121\n","  -0.29536023 -0.02704213 -0.03382296]\n"," [ 0.19705619  0.44132225  0.04587733  0.21678365  0.24679883  0.0339953\n","   0.         -0.31003637 -0.27734272 -0.15151692 -0.30458906 -0.12015914\n","   0.06635736 -0.12294522  0.20034957]\n"," [-0.14626492 -0.55944315 -0.12207047 -0.36616225 -0.45983477  0.16103661\n","  -0.31003637  0.          0.67129803  0.6421734   0.37562136  0.11106249\n","  -0.26767842  0.11614562 -0.13333569]\n"," [-0.20716272 -0.61724242 -0.50220895 -0.53407179 -0.57815033 -0.08961375\n","  -0.27734272  0.67129803  0.          0.51279559  0.11535814 -0.03756992\n","   0.20547963  0.0098681   0.20656848]\n"," [-0.01932991 -0.10934934 -0.05944264 -0.16535064 -0.17797306  0.00490449\n","  -0.15151692  0.6421734   0.51279559  0.          0.20467727 -0.17641335\n","  -0.11057092 -0.15926215  0.05488226]\n"," [-0.15652164 -0.25394405  0.08870163 -0.03218564  0.07242114 -0.14571103\n","  -0.30458906  0.37562136  0.11535814  0.20467727  0.          0.46591988\n","   0.0526923   0.38955475 -0.27183095]\n"," [-0.28049982 -0.15493399  0.1442684   0.05986651  0.08410655  0.13506121\n","  -0.12015914  0.11106249 -0.03756992 -0.17641335  0.46591988  0.\n","   0.1135386   0.98479028 -0.12751266]\n"," [-0.17138803 -0.18232735 -0.41469532 -0.11841703 -0.13201032 -0.29536023\n","   0.06635736 -0.26767842  0.20547963 -0.11057092  0.0526923   0.1135386\n","   0.          0.2404665   0.54649198]\n"," [-0.17873779 -0.12858349  0.02158323 -0.09234453 -0.1261167  -0.02704213\n","  -0.12294522  0.11614562  0.0098681  -0.15926215  0.38955475  0.98479028\n","   0.2404665   0.         -0.16815678]\n"," [-0.05382542 -0.11987415 -0.22569421 -0.26826495 -0.20566083 -0.03382296\n","   0.20034957 -0.13333569  0.20656848  0.05488226 -0.27183095 -0.12751266\n","   0.54649198 -0.16815678  0.        ]]\n","(15, 15)\n"]}]},{"cell_type":"code","source":["#threshholding the connectivity graph\n","\n","def threshold_graph(A, keep_fraction=0.4):\n","    triu_indices = np.triu_indices_from(A, k=1)                     #get upper triangle indices above the diagonal\n","    edge_values = A[triu_indices]                                   #get all edge values of upper triangle values\n","    threshold = np.percentile(edge_values, 100*(1-keep_fraction))   #60% weakest means keep top 40%\n","    mask = A >= threshold\n","    A_thresh = A * mask\n","    A_thresh = np.maximum(A_thresh, A_thresh.T)                     #ensure symmetry\n","    return A_thresh\n","\n","\n","for sid, graphs in graphs_by_sid.items():\n","    graphs_by_sid[sid]['abs'] = threshold_graph(graphs['abs'], keep_fraction=0.1)\n","    graphs_by_sid[sid]['rel'] = threshold_graph(graphs['rel'], keep_fraction=0.1)\n","\n","\n","\n"],"metadata":{"id":"5NjBrTvN5edL","executionInfo":{"status":"ok","timestamp":1755797487558,"user_tz":420,"elapsed":80,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"collapsed":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#adding to dictionary demographic data and labels.\n","\n","\n","demo_cols = ['Age', 'handedness', 'sex', 'PrimaryEthnicity', 'PrimaryRace','Education','Parental Education']\n","outcome = ['Imp20PercentBPRS']\n","df_labels = pd.read_excel(file_path, sheet_name='outcomeanddemographics', skiprows=1)\n","\n","demo_dict = df_labels.set_index('SID')[demo_cols].to_dict(orient='index')\n","\n","graphs_by_sid = {}\n","\n","for sid, abs_mat, rel_mat in zip(df['SID'], abs_adj_matrices, rel_adj_matrices):\n","    demos = demo_dict.get(sid, None)\n","    graphs_by_sid[sid] = {\n","        'abs': abs_mat,\n","        'rel': rel_mat,\n","        'demo': demos,\n","        'outcome': df_labels.loc[df_labels['SID'] == sid, outcome].values[0][0] if sid in df_labels['SID'].values else None\n","    }\n","\n","#print(graphs_by_sid['epp270'])  # Relative graph\n","\n","graphs_by_sid = {\n","    sid: graph\n","    for sid, graph in graphs_by_sid.items()\n","    if graph.get('outcome') is not None\n","}\n","\n","\n","\n","outcome_count = sum(1 for graph in graphs_by_sid.values() if graph.get('outcome') == 1)\n","outcome_count_zero = sum(1 for graph in graphs_by_sid.values() if graph.get('outcome') == 0)\n","print(f\"Number of instances with outcome == 1: {outcome_count}\")\n","print(f\"Number of instances with outcome == 0: {outcome_count_zero}\")\n","\n","\n","#Number of instances with outcome == 1: 29\n","#Number of instances with outcome == 0: 27"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Th0aensz5fHY","executionInfo":{"status":"ok","timestamp":1755797489846,"user_tz":420,"elapsed":459,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"ad9100ca-36f5-4a01-c958-6a5b0fced0aa"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of instances with outcome == 1: 29\n","Number of instances with outcome == 0: 27\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy.linalg import eigh\n","\n","def spectral_embedding(A, k=5):\n","    D = np.sum(A, axis=1)\n","    D_inv_sqrt = np.diag(1.0 / np.sqrt(D + 1e-10))\n","    L_sym = np.eye(len(A)) - D_inv_sqrt @ A @ D_inv_sqrt\n","    eigenvalues, eigenvectors = eigh(L_sym)\n","    return eigenvectors[:, 1:k+1]  # node embeddings shape (N_nodes, k)\n","\n","for sid, graph in graphs_by_sid.items():\n","    A = graph['abs']  # or graph['rel']\n","    node_emb = spectral_embedding(A, k=5)  # shape (N_nodes, 5)\n","#    print(f\"Graph {sid}: node_emb shape before pooling: {node_emb.shape}\")\n","\n","    graph_emb = np.mean(node_emb, axis=0)  # shape (5,)\n","#    print(f\"Graph {sid}: graph_emb shape after pooling: {graph_emb.shape}\")\n","\n","    graphs_by_sid[sid]['graph_embedding'] = graph_emb\n","\n","\n","print(graphs_by_sid['epp270']['graph_embedding'].shape)  # Should print (5,)\n","\n","\n","# #\n","# Graph epp549: graph_emb shape after pooling: (5,)\n","# Graph epp551: node_emb shape before pooling: (15, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9qImPx-5svB","executionInfo":{"status":"ok","timestamp":1755797497365,"user_tz":420,"elapsed":19,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"223d68c3-52d9-43cd-e2b2-f12f85a9da79"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(5,)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","class EmbeddingDataset(Dataset):\n","    def __init__(self, sids, graphs_by_sid):\n","        self.sids = sids\n","        self.graphs_by_sid = graphs_by_sid\n","\n","    def __len__(self):\n","        return len(self.sids)\n","\n","    def __getitem__(self, idx):\n","        sid = self.sids[idx]\n","        embedding = torch.tensor(self.graphs_by_sid[sid]['graph_embedding'], dtype=torch.float)\n","        label = torch.tensor(self.graphs_by_sid[sid]['outcome'], dtype=torch.long)\n","        return embedding, label\n"],"metadata":{"id":"IJMx4kbL6q8V","executionInfo":{"status":"ok","timestamp":1755797510985,"user_tz":420,"elapsed":5886,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import numpy as np\n","\n","def mixup_embeddings(x1, y1, x2, y2, alpha=1.0, num_classes=2):\n","    lam = np.random.beta(alpha, alpha)\n","    x_mix = lam * x1 + (1 - lam) * x2\n","    y1_onehot = F.one_hot(y1, num_classes=num_classes).float()\n","    y2_onehot = F.one_hot(y2, num_classes=num_classes).float()\n","    y_mix = lam * y1_onehot + (1 - lam) * y2_onehot\n","    return x_mix, y_mix\n"],"metadata":{"id":"HkFy5Q1N7qE2","executionInfo":{"status":"ok","timestamp":1755797533463,"user_tz":420,"elapsed":44,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MLPClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_classes):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","\n","class BiggerMLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_classes):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n"],"metadata":{"id":"qN-HhTgH9HQp","executionInfo":{"status":"ok","timestamp":1755797536704,"user_tz":420,"elapsed":3,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaeb6bb9","executionInfo":{"status":"ok","timestamp":1755797542063,"user_tz":420,"elapsed":7,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"source":["from torch.utils.data import DataLoader\n","\n","def train_one_epoch(model, optimizer, dataloader, device, alpha=0.0, num_classes=2):\n","    model.train()\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    for embeddings, labels in dataloader:\n","        embeddings, labels = embeddings.to(device), labels.to(device)\n","\n","        indices = torch.randperm(embeddings.size(0))\n","        x2 = embeddings[indices]\n","        y2 = labels[indices]\n","\n","        x_mix, y_mix = mixup_embeddings(embeddings, labels, x2, y2, alpha, num_classes)\n","        #x_mix, y_mix = x2,y2\n","\n","        outputs = model(x_mix)\n","        loss = F.binary_cross_entropy_with_logits(outputs, y_mix)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        preds = outputs.argmax(dim=1)          # shape (batch_size,)\n","        true_labels = y_mix.argmax(dim=1)      # shape (batch_size,)\n","\n","\n","        true_labels = y_mix.argmax(dim=1)\n","\n","        all_preds.append(preds.cpu())\n","        all_labels.append(true_labels.cpu())\n","\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","    train_acc = (all_preds == all_labels).float().mean().item()\n","\n","    return total_loss / len(dataloader), train_acc\n","\n","\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def evaluate(model, dataloader, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for embeddings, labels in dataloader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","            outputs = model(embeddings)  # logits shape: (batch_size, 2)\n","            preds = outputs.argmax(dim=1)  # predicted class 0 or 1\n","\n","            all_preds.append(preds.cpu())\n","            all_labels.append(labels.cpu())\n","\n","    all_preds = torch.cat(all_preds).numpy()\n","    all_labels = torch.cat(all_labels).numpy()\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds)  # default binary average\n","\n","    return acc, f1\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","sids = list(graphs_by_sid.keys())\n","labels = np.array([graphs_by_sid[sid]['outcome'] for sid in sids])\n","num_classes = len(set(labels))\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","avg_acc = 0\n","avg_f1 = 0\n","\n","for fold, (train_idx, test_idx) in enumerate(skf.split(sids, labels)):\n","    print(f\"Fold {fold + 1}\")\n","\n","    train_sids = [sids[i] for i in train_idx]\n","    test_sids = [sids[i] for i in test_idx]\n","\n","    train_dataset = EmbeddingDataset(train_sids, graphs_by_sid)\n","    test_dataset = EmbeddingDataset(test_sids, graphs_by_sid)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=8)\n","\n","    model = BiggerMLP(input_dim=graphs_by_sid[train_sids[0]]['graph_embedding'].shape[0],\n","                          hidden_dim=64, num_classes=num_classes).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","    for epoch in range(1, 10):\n","        loss, train_acc = train_one_epoch(model, optimizer, train_loader, device, alpha=1.0, num_classes=num_classes)\n","        print(f\"Epoch {epoch} Loss: {loss:.4f} Train Accuracy: {train_acc:.4f}\")\n","\n","    test_acc, test_f1 = evaluate(model, test_loader, device)\n","    print(f\"Fold {fold + 1} Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}\")\n","\n","    avg_acc += test_acc\n","    avg_f1 += test_f1\n","\n","# Average across folds\n","avg_acc /= skf.get_n_splits()\n","avg_f1 /= skf.get_n_splits()\n","\n","print(f\"\\nAverage Test Accuracy across folds: {avg_acc:.4f}\")\n","print(f\"Average Test F1 Score across folds: {avg_f1:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5C3b-QtCqw8","executionInfo":{"status":"ok","timestamp":1755797604270,"user_tz":420,"elapsed":1507,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"aa052a40-fdb5-4a52-9053-483cfaa51964"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n","Epoch 1 Loss: 0.7001 Train Accuracy: 0.4318\n","Epoch 2 Loss: 0.6938 Train Accuracy: 0.5227\n","Epoch 3 Loss: 0.6941 Train Accuracy: 0.5227\n","Epoch 4 Loss: 0.6926 Train Accuracy: 0.5227\n","Epoch 5 Loss: 0.6956 Train Accuracy: 0.5227\n","Epoch 6 Loss: 0.6949 Train Accuracy: 0.5227\n","Epoch 7 Loss: 0.6915 Train Accuracy: 0.5227\n","Epoch 8 Loss: 0.6940 Train Accuracy: 0.5227\n","Epoch 9 Loss: 0.6925 Train Accuracy: 0.5227\n","Fold 1 Test Accuracy: 0.5000, F1 Score: 0.6667\n","Fold 2\n","Epoch 1 Loss: 0.7007 Train Accuracy: 0.4444\n","Epoch 2 Loss: 0.6926 Train Accuracy: 0.5556\n","Epoch 3 Loss: 0.6969 Train Accuracy: 0.5111\n","Epoch 4 Loss: 0.6924 Train Accuracy: 0.5111\n","Epoch 5 Loss: 0.6926 Train Accuracy: 0.5111\n","Epoch 6 Loss: 0.6928 Train Accuracy: 0.5111\n","Epoch 7 Loss: 0.6930 Train Accuracy: 0.5111\n","Epoch 8 Loss: 0.6939 Train Accuracy: 0.5111\n","Epoch 9 Loss: 0.6933 Train Accuracy: 0.5111\n","Fold 2 Test Accuracy: 0.5455, F1 Score: 0.7059\n","Fold 3\n","Epoch 1 Loss: 0.7013 Train Accuracy: 0.4889\n","Epoch 2 Loss: 0.6944 Train Accuracy: 0.5111\n","Epoch 3 Loss: 0.6931 Train Accuracy: 0.5111\n","Epoch 4 Loss: 0.6939 Train Accuracy: 0.5111\n","Epoch 5 Loss: 0.6937 Train Accuracy: 0.4889\n","Epoch 6 Loss: 0.6937 Train Accuracy: 0.4889\n","Epoch 7 Loss: 0.6934 Train Accuracy: 0.4889\n","Epoch 8 Loss: 0.6935 Train Accuracy: 0.4889\n","Epoch 9 Loss: 0.6935 Train Accuracy: 0.4889\n","Fold 3 Test Accuracy: 0.4545, F1 Score: 0.0000\n","Fold 4\n","Epoch 1 Loss: 0.6957 Train Accuracy: 0.4444\n","Epoch 2 Loss: 0.6946 Train Accuracy: 0.5111\n","Epoch 3 Loss: 0.7025 Train Accuracy: 0.5111\n","Epoch 4 Loss: 0.6936 Train Accuracy: 0.5111\n","Epoch 5 Loss: 0.6955 Train Accuracy: 0.5111\n","Epoch 6 Loss: 0.6930 Train Accuracy: 0.5111\n","Epoch 7 Loss: 0.6934 Train Accuracy: 0.5111\n","Epoch 8 Loss: 0.6928 Train Accuracy: 0.5111\n","Epoch 9 Loss: 0.6927 Train Accuracy: 0.5111\n","Fold 4 Test Accuracy: 0.5455, F1 Score: 0.7059\n","Fold 5\n","Epoch 1 Loss: 0.6961 Train Accuracy: 0.4000\n","Epoch 2 Loss: 0.6903 Train Accuracy: 0.5333\n","Epoch 3 Loss: 0.6907 Train Accuracy: 0.5333\n","Epoch 4 Loss: 0.6911 Train Accuracy: 0.5333\n","Epoch 5 Loss: 0.6906 Train Accuracy: 0.5333\n","Epoch 6 Loss: 0.6915 Train Accuracy: 0.5333\n","Epoch 7 Loss: 0.6926 Train Accuracy: 0.5333\n","Epoch 8 Loss: 0.6927 Train Accuracy: 0.5333\n","Epoch 9 Loss: 0.6890 Train Accuracy: 0.5333\n","Fold 5 Test Accuracy: 0.4545, F1 Score: 0.6250\n","\n","Average Test Accuracy across folds: 0.5000\n","Average Test F1 Score across folds: 0.5407\n"]}]}]}