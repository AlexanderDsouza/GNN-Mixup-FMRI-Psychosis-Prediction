{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Core libraries\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","from scipy.linalg import eigh\n","\n","# ===========================\n","# Load and preprocess dataset\n","# ===========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjLFZcSjxBVr","executionInfo":{"status":"ok","timestamp":1757530013567,"user_tz":420,"elapsed":598,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"d17865f5-0783-4f85-df65-724dc0e86f3b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"hkSE2CWmwZU7","executionInfo":{"status":"ok","timestamp":1757530011261,"user_tz":420,"elapsed":12,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def load_schaefer_graphs(\n","    keep_fraction=0.3,\n","    outcome_col='Imp20PercentBPRS'\n","):\n","    # ===========================\n","    # Hardcoded file names\n","    # ===========================\n","    graph_file = \"/content/drive/Shared drives/GNN/SchaeferAtlas_Rest_Results_Freq_008to09_wholebrain.xlsx\"\n","    outcome_file = \"/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx\"\n","\n","    demo_cols = ['Age', 'handedness', 'sex', 'PrimaryEthnicity',\n","                 'PrimaryRace', 'Education', 'Parental Education']\n","\n","    # ===========================\n","    # Load outcome + demographics\n","    # ===========================\n","    df_labels = pd.read_excel(outcome_file, sheet_name='outcomeanddemographics', skiprows=1)\n","    demo_dict = df_labels.set_index('SID')[demo_cols].to_dict(orient='index')\n","\n","    # Helper: threshold adjacency\n","    def threshold_graph(A, keep_fraction):\n","        triu_indices = np.triu_indices_from(A, k=1)\n","        edge_values = A[triu_indices]\n","        edge_values = edge_values[edge_values > 0]  # ignore zeros\n","        if len(edge_values) == 0:\n","            return np.zeros_like(A)\n","        threshold = np.percentile(edge_values, 100 * (1 - keep_fraction))\n","        mask = A >= threshold\n","        A_thresh = A * mask\n","        A_thresh = np.maximum(A_thresh, A_thresh.T)  # enforce symmetry\n","        return A_thresh\n","\n","    # ===========================\n","    # Load only needed sheets\n","    # ===========================\n","    xl = pd.ExcelFile(graph_file)\n","    available_sheets = set(xl.sheet_names)\n","\n","    # Get SIDs that actually have outcomes\n","    sids_with_outcome = df_labels['SID'].dropna().unique()\n","    sids_to_load = [sid for sid in sids_with_outcome if sid in available_sheets]\n","\n","    graphs_by_sid = {}\n","\n","    for sid in sids_to_load:\n","        try:\n","            # Load matrix for this patient\n","            mat_df = pd.read_excel(graph_file, sheet_name=sid, index_col=0)\n","            mat = mat_df.values.astype(float)\n","\n","            mat = mat_df.values.astype(float)\n","\n","            # Replace NaNs and infs with 0\n","            mat = np.nan_to_num(mat, nan=0.0, posinf=0.0, neginf=0.0)\n","            rel = mat\n","            # Take absolute value (since matrices can have negatives)\n","            mat = np.abs(mat)\n","\n","            # Take absolute values (since matrices aren't absolute in this dataset)\n","\n","\n","            # Get outcome + demographics\n","            row = df_labels.loc[df_labels['SID'] == sid]\n","            outcome = row[outcome_col].values[0]\n","            demos = demo_dict.get(sid, None)\n","\n","            # Threshold adjacency\n","            mat_thresh = threshold_graph(mat, keep_fraction)\n","            rel_thresh = threshold_graph(rel, keep_fraction)\n","\n","            graphs_by_sid[sid] = {\n","                'abs': mat_thresh,      # absolute-valued adjacency\n","                'rel': rel_thresh,\n","                'demo': demos,\n","                'outcome': outcome,\n","                'nodes': list(mat_df.index)  # preserve brain region labels\n","            }\n","        except Exception as e:\n","            print(f\"Skipping SID {sid} due to error: {e}\")\n","            continue\n","\n","    return graphs_by_sid\n","\n"]},{"cell_type":"code","source":["#combines all the previous dataloading\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","\n","from itertools import product\n","import numpy as np\n","\n","def mean_between_group_corr(responders_idx, nonresponders_idx):\n","    corr_values = []\n","    # Compare every responder with every non-responder\n","    for i, j in product(responders_idx, nonresponders_idx):\n","        vec_i = demo_matrix[i]\n","        vec_j = demo_matrix[j]\n","        # Skip if either vector has zero variance\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    if len(corr_values) == 0:\n","        return np.nan\n","    return np.mean(corr_values)\n","\n","def load_and_preprocess_graphs(\n","    file_path,\n","    abs_keep_fraction=0.3,\n","    rel_keep_fraction=0.3,\n","    demo_cols=None,\n","    outcome_col='Imp20PercentBPRS'\n","):\n","    if demo_cols is None:\n","        demo_cols = ['Age', 'handedness', 'sex', 'PrimaryEthnicity', 'PrimaryRace', 'Education', 'Parental Education']\n","\n","    # ===========================\n","    # Load dataset\n","    # ===========================\n","    df = pd.read_excel(file_path)\n","    df_labels = pd.read_excel(file_path, sheet_name='outcomeanddemographics', skiprows=1)\n","\n","    # ===========================\n","    # Extract nodes and edges\n","    # ===========================\n","    edge_columns = df.columns.drop('SID')\n","\n","    # Get all unique nodes\n","    nodes_set = set()\n","    for col in edge_columns:\n","        raw_node1, raw_node2 = col.split('-')\n","        node1 = raw_node1.replace('ABS_', '')\n","        node2 = raw_node2.replace('ABS_', '')\n","        nodes_set.add(node1)\n","        nodes_set.add(node2)\n","\n","    nodes = sorted(list(nodes_set))\n","    n_nodes = len(nodes)\n","    node_to_idx = {node: i for i, node in enumerate(nodes)}\n","\n","    abs_edge_cols, rel_edge_cols = [], []\n","    abs_edge_to_idx, rel_edge_to_idx = [], []\n","\n","    for col in edge_columns:\n","        raw_node1, raw_node2 = col.split('-')\n","        node1 = raw_node1.replace('ABS_', '')\n","        node2 = raw_node2.replace('ABS_', '')\n","        idx1, idx2 = node_to_idx[node1], node_to_idx[node2]\n","\n","        if 'ABS_' in col:\n","            abs_edge_cols.append(col)\n","            abs_edge_to_idx.append((idx1, idx2))\n","        else:\n","            rel_edge_cols.append(col)\n","            rel_edge_to_idx.append((idx1, idx2))\n","\n","    # ===========================\n","    # Build adjacency matrices\n","    # ===========================\n","    abs_adj_matrices, rel_adj_matrices = [], []\n","\n","    for _, row in df.iterrows():\n","        abs_adj = np.zeros((n_nodes, n_nodes))\n","        rel_adj = np.zeros((n_nodes, n_nodes))\n","\n","        for col_idx, (i1, i2) in enumerate(abs_edge_to_idx):\n","            val = row[abs_edge_cols[col_idx]]\n","            abs_adj[i1, i2] = val\n","            abs_adj[i2, i1] = val\n","\n","        for col_idx, (i1, i2) in enumerate(rel_edge_to_idx):\n","            val = row[rel_edge_cols[col_idx]]\n","            rel_adj[i1, i2] = val\n","            rel_adj[i2, i1] = val\n","\n","        abs_adj_matrices.append(abs_adj)\n","        rel_adj_matrices.append(rel_adj)\n","\n","    # ===========================\n","    # Thresholding function\n","    # ===========================\n","    def threshold_graph(A, keep_fraction):\n","        triu_indices = np.triu_indices_from(A, k=1)\n","        edge_values = A[triu_indices]\n","        threshold = np.percentile(edge_values, 100 * (1 - keep_fraction))\n","        mask = A >= threshold\n","        A_thresh = A * mask\n","        A_thresh = np.maximum(A_thresh, A_thresh.T)\n","        return A_thresh\n","\n","    # ===========================\n","    # Build final dictionary\n","    # ===========================\n","    demo_dict = df_labels.set_index('SID')[demo_cols].to_dict(orient='index')\n","\n","    graphs_by_sid = {}\n","    for sid, abs_mat, rel_mat in zip(df['SID'], abs_adj_matrices, rel_adj_matrices):\n","        demos = demo_dict.get(sid, None)\n","        abs_mat_thresh = threshold_graph(abs_mat, abs_keep_fraction)\n","        rel_mat_thresh = threshold_graph(rel_mat, rel_keep_fraction)\n","        outcome = df_labels.loc[df_labels['SID'] == sid, outcome_col].values[0] \\\n","                  if sid in df_labels['SID'].values else None\n","\n","        graphs_by_sid[sid] = {\n","            'abs': abs_mat_thresh,\n","            'rel': rel_mat_thresh,\n","            'demo': demos,\n","            'outcome': outcome\n","        }\n","\n","    # Filter out subjects without outcome\n","    graphs_by_sid = {sid: g for sid, g in graphs_by_sid.items() if g['outcome'] is not None}\n","\n","    return graphs_by_sid\n"],"metadata":{"id":"0_TRRcXOxHQf","executionInfo":{"status":"ok","timestamp":1757528679013,"user_tz":420,"elapsed":22,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from itertools import combinations\n","\n","# ===========================\n","# Load labels and demographics\n","# ===========================\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","\n","#graphs_by_sid = load_schaefer_graphs(keep_fraction=0.3)\n","\n","# ===========================\n","# Load labels and demographics\n","# ===========================\n","df_labels = pd.read_excel(file_path, sheet_name='outcomeanddemographics', skiprows=1)\n","\n","# Impute missing Parental Education with mean\n","parent_edu = df_labels['Parental Education'].values.astype(float)\n","mean_val = np.nanmean(parent_edu)\n","parent_edu[np.isnan(parent_edu)] = mean_val\n","df_labels['Parental Education'] = parent_edu\n","\n","# ===========================\n","# Build demo matrix\n","# ===========================\n","sids = list(graphs_by_sid.keys())\n","demo_list = []\n","outcomes = []\n","\n","for sid in sids:\n","    demo_dict = graphs_by_sid[sid]['demo']\n","    demo_vector = [\n","        demo_dict['Age'],\n","        demo_dict['handedness'],\n","        demo_dict['sex'],\n","        demo_dict['PrimaryEthnicity'],\n","        demo_dict['PrimaryRace'],\n","        demo_dict['Education'],\n","        demo_dict['Parental Education']\n","    ]\n","    demo_list.append(demo_vector)\n","    outcomes.append(graphs_by_sid[sid]['outcome'])\n","\n","demo_matrix = np.array(demo_list, dtype=float)\n","outcomes = np.array(outcomes)\n","\n","# ===========================\n","# Remove zero-variance columns\n","# ===========================\n","stds = demo_matrix.std(axis=0)\n","nonzero_var_cols = stds > 0\n","demo_matrix = demo_matrix[:, nonzero_var_cols]\n","\n","# ===========================\n","# Split indices by outcome\n","# ===========================\n","responders_idx = np.where(outcomes == 1)[0]  # adjust to your encoding\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","# ===========================\n","# Mean pairwise correlation\n","# ===========================\n","def mean_pairwise_corr(indices):\n","    corr_values = []\n","    for i, j in combinations(indices, 2):\n","        vec_i = demo_matrix[i]\n","        vec_j = demo_matrix[j]\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    if len(corr_values) == 0:\n","        return np.nan\n","    return np.mean(corr_values)\n","\n","mean_corr_responders = mean_pairwise_corr(responders_idx)\n","mean_corr_nonresponders = mean_pairwise_corr(nonresponders_idx)\n","\n","print(\"Mean correlation - responders:\", mean_corr_responders)\n","print(\"Mean correlation - non-responders:\", mean_corr_nonresponders)\n","\n","# ===========================\n","# Average of each demographic feature\n","# ===========================\n","feature_names = ['Age','handedness','sex','PrimaryEthnicity','PrimaryRace','Education','Parental Education']\n","\n","mean_responders = np.mean(demo_matrix[responders_idx], axis=0)\n","mean_nonresponders = np.mean(demo_matrix[nonresponders_idx], axis=0)\n","\n","print(\"\\nMean demographics - responders:\")\n","for name, val in zip(feature_names, mean_responders):\n","    print(f\"{name}: {val:.2f}\")\n","\n","print(\"\\nMean demographics - non-responders:\")\n","for name, val in zip(feature_names, mean_nonresponders):\n","    print(f\"{name}: {val:.2f}\")\n","\n","\n","mean_corr_between = mean_between_group_corr(responders_idx, nonresponders_idx)\n","print(\"Mean correlation between responders and non-responders:\", mean_corr_between)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWGUjHuK1JOw","executionInfo":{"status":"ok","timestamp":1757371696426,"user_tz":420,"elapsed":5691,"user":{"displayName":"Davidson Lab","userId":"15145557194024733869"}},"outputId":"95e02fc8-bf4d-4e1d-de19-adf5b8fec823"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean correlation - responders: 0.9868648579820746\n","Mean correlation - non-responders: 0.9873388791828345\n","\n","Mean demographics - responders:\n","Age: 19.69\n","handedness: 1.10\n","sex: 1.21\n","PrimaryEthnicity: 0.28\n","PrimaryRace: 3.24\n","Education: 11.83\n","\n","Mean demographics - non-responders:\n","Age: 18.93\n","handedness: 1.04\n","sex: 1.56\n","PrimaryEthnicity: 0.33\n","PrimaryRace: 3.89\n","Education: 11.78\n","Mean correlation between responders and non-responders: 0.9864991697868103\n"]}]},{"cell_type":"code","source":["def flatten_upper_tri(mat):\n","    # Take only upper triangle (excluding diagonal)\n","    triu_indices = np.triu_indices_from(mat, k=1)\n","    return mat[triu_indices]\n","sids = list(graphs_by_sid.keys())\n","outcomes = []\n","graph_vectors = []\n","\n","for sid in sids:\n","    adj = graphs_by_sid[sid]['abs']  # or 'rel' depending on which you want\n","    vec = flatten_upper_tri(adj)\n","    graph_vectors.append(vec)\n","    outcomes.append(graphs_by_sid[sid]['outcome'])\n","\n","graph_matrix = np.array(graph_vectors)  # shape: (n_patients, n_edges)\n","outcomes = np.array(outcomes)\n","\n","from itertools import combinations\n","\n","def mean_pairwise_corr(matrix, indices):\n","    corr_values = []\n","    for i, j in combinations(indices, 2):\n","        vec_i = matrix[i]\n","        vec_j = matrix[j]\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    if len(corr_values) == 0:\n","        return np.nan\n","    return np.mean(corr_values)\n","\n","# Split indices\n","responders_idx = np.where(outcomes == 1)[0]\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","mean_corr_responders = mean_pairwise_corr(graph_matrix, responders_idx)\n","mean_corr_nonresponders = mean_pairwise_corr(graph_matrix, nonresponders_idx)\n","\n","print(\"Mean correlation - responders:\", mean_corr_responders)\n","print(\"Mean correlation - non-responders:\", mean_corr_nonresponders)\n","\n","from itertools import product\n","\n","corr_values = []\n","responders_idx = np.where(outcomes == 1)[0]\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","responders_matrix = graph_matrix[responders_idx]\n","nonresponders_matrix = graph_matrix[nonresponders_idx]\n","\n","# Compare every responder with every non-responder\n","for i, j in product(range(responders_matrix.shape[0]), range(nonresponders_matrix.shape[0])):\n","    vec_i = responders_matrix[i]\n","    vec_j = nonresponders_matrix[j]\n","\n","    if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","        continue  # skip if vector has zero variance\n","\n","    corr = np.corrcoef(vec_i, vec_j)[0,1]\n","    if not np.isnan(corr):\n","        corr_values.append(corr)\n","\n","# Mean between-group correlation\n","mean_corr_between = np.mean(corr_values)\n","print(\"Mean correlation between responders and non-responders:\", mean_corr_between)\n","\n","#Pearson correlation coefficient"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWCx4OgA2W0c","executionInfo":{"status":"ok","timestamp":1757371702195,"user_tz":420,"elapsed":333,"user":{"displayName":"Davidson Lab","userId":"15145557194024733869"}},"outputId":"9e179750-9f81-4e96-e4b9-d460f0b9e9ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean correlation - responders: 0.3815547586741385\n","Mean correlation - non-responders: 0.3961451769994697\n","Mean correlation between responders and non-responders: 0.3876776424107272\n"]}]},{"cell_type":"code","source":["def flatten_upper_tri(mat):\n","    # Take only upper triangle (excluding diagonal)\n","    triu_indices = np.triu_indices_from(mat, k=1)\n","    return mat[triu_indices]\n","sids = list(graphs_by_sid.keys())\n","outcomes = []\n","graph_vectors = []\n","\n","for sid in sids:\n","    adj = graphs_by_sid[sid]['rel']  # or 'rel' depending on which you want\n","    vec = flatten_upper_tri(adj)\n","    graph_vectors.append(vec)\n","    outcomes.append(graphs_by_sid[sid]['outcome'])\n","\n","graph_matrix = np.array(graph_vectors)  # shape: (n_patients, n_edges)\n","outcomes = np.array(outcomes)\n","\n","from itertools import combinations\n","\n","def mean_pairwise_corr(matrix, indices):\n","    corr_values = []\n","    for i, j in combinations(indices, 2):\n","        vec_i = matrix[i]\n","        vec_j = matrix[j]\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    if len(corr_values) == 0:\n","        return np.nan\n","    return np.mean(corr_values)\n","\n","# Split indices\n","responders_idx = np.where(outcomes == 1)[0]\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","mean_corr_responders = mean_pairwise_corr(graph_matrix, responders_idx)\n","mean_corr_nonresponders = mean_pairwise_corr(graph_matrix, nonresponders_idx)\n","\n","print(\"rel Mean correlation - responders:\", mean_corr_responders)\n","print(\"rel Mean correlation - non-responders:\", mean_corr_nonresponders)\n","\n","from itertools import product\n","\n","corr_values = []\n","responders_idx = np.where(outcomes == 1)[0]\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","responders_matrix = graph_matrix[responders_idx]\n","nonresponders_matrix = graph_matrix[nonresponders_idx]\n","\n","# Compare every responder with every non-responder\n","for i, j in product(range(responders_matrix.shape[0]), range(nonresponders_matrix.shape[0])):\n","    vec_i = responders_matrix[i]\n","    vec_j = nonresponders_matrix[j]\n","\n","    if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","        continue  # skip if vector has zero variance\n","\n","    corr = np.corrcoef(vec_i, vec_j)[0,1]\n","    if not np.isnan(corr):\n","        corr_values.append(corr)\n","\n","# Mean between-group correlation\n","mean_corr_between = np.mean(corr_values)\n","print(\"rel Mean correlation between responders and non-responders:\", mean_corr_between)\n","\n","#Pearson correlation coefficient"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7svde-VB8RyK","executionInfo":{"status":"ok","timestamp":1757371707696,"user_tz":420,"elapsed":279,"user":{"displayName":"Davidson Lab","userId":"15145557194024733869"}},"outputId":"59ca60cb-9561-4fc7-9858-da26b8652716"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rel Mean correlation - responders: 0.5222061973246399\n","rel Mean correlation - non-responders: 0.4945713722099749\n","rel Mean correlation between responders and non-responders: 0.5052354151151565\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from itertools import combinations, product\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# ===========================\n","# 1. Mean pairwise correlations\n","# ===========================\n","def mean_pairwise_corr(matrix, indices):\n","    corr_values = []\n","    for i, j in combinations(indices, 2):\n","        vec_i, vec_j = matrix[i], matrix[j]\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    return np.mean(corr_values) if corr_values else np.nan\n","\n","def mean_between_group_corr(matrix, idx1, idx2):\n","    corr_values = []\n","    for i, j in product(idx1, idx2):\n","        vec_i, vec_j = matrix[i], matrix[j]\n","        if np.std(vec_i) == 0 or np.std(vec_j) == 0:\n","            continue\n","        corr = np.corrcoef(vec_i, vec_j)[0,1]\n","        if not np.isnan(corr):\n","            corr_values.append(corr)\n","    return np.mean(corr_values) if corr_values else np.nan\n","\n","responders_idx = np.where(outcomes == 1)[0]\n","nonresponders_idx = np.where(outcomes == 0)[0]\n","\n","mean_corr_resp = mean_pairwise_corr(graph_matrix, responders_idx)\n","mean_corr_nonresp = mean_pairwise_corr(graph_matrix, nonresponders_idx)\n","mean_corr_between = mean_between_group_corr(graph_matrix, responders_idx, nonresponders_idx)\n","\n","print(\"Mean correlation - responders:\", mean_corr_resp)\n","print(\"Mean correlation - non-responders:\", mean_corr_nonresp)\n","print(\"Mean correlation between groups:\", mean_corr_between)\n","\n","# ===========================\n","# 2. Permutation test for correlations\n","# ===========================\n","def perm_test_corr(matrix, outcomes, n_permutations=1000):\n","    observed_within = (\n","        mean_pairwise_corr(matrix, np.where(outcomes==1)[0]) -\n","        mean_pairwise_corr(matrix, np.where(outcomes==0)[0])\n","    )\n","    perm_diffs = []\n","    for _ in range(n_permutations):\n","        perm_labels = np.random.permutation(outcomes)\n","        perm_diff = (\n","            mean_pairwise_corr(matrix, np.where(perm_labels==1)[0]) -\n","            mean_pairwise_corr(matrix, np.where(perm_labels==0)[0])\n","        )\n","        perm_diffs.append(perm_diff)\n","    perm_diffs = np.array(perm_diffs)\n","    p_val = np.mean(np.abs(perm_diffs) >= np.abs(observed_within))\n","    return observed_within, p_val\n","\n","obs_diff, p_val_corr = perm_test_corr(graph_matrix, outcomes)\n","print(\"Observed within-group difference:\", obs_diff)\n","print(\"Permutation p-value (within-group correlation difference):\", p_val_corr)\n","\n","# ===========================\n","# 3. Edge-wise correlation with outcome\n","# ===========================\n","edge_corrs = np.array([np.corrcoef(graph_matrix[:,i], outcomes)[0,1]\n","                       for i in range(graph_matrix.shape[1])])\n","top_edges = np.argsort(np.abs(edge_corrs))[::-1][:20]  # top 20 edges\n","print(\"Top 20 edges by absolute correlation:\", top_edges)\n","print(\"Corresponding correlations:\", edge_corrs[top_edges])\n","\n","# ===========================\n","# 4. Supervised classification with cross-validation\n","# ===========================\n","X = graph_matrix\n","y = outcomes\n","clf = LogisticRegression(max_iter=1000)\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","accs = []\n","for train_idx, test_idx in skf.split(X, y):\n","    clf.fit(X[train_idx], y[train_idx])\n","    preds = clf.predict(X[test_idx])\n","    accs.append(accuracy_score(y[test_idx], preds))\n","mean_acc = np.mean(accs)\n","print(\"5-fold CV accuracy:\", mean_acc)\n","\n","# ===========================\n","# 5. Permutation test for classifier\n","# ===========================\n","n_perm = 1000\n","perm_accs = []\n","for _ in range(n_perm):\n","    y_perm = np.random.permutation(y)\n","    perm_fold_accs = []\n","    for train_idx, test_idx in skf.split(X, y_perm):\n","        clf.fit(X[train_idx], y_perm[train_idx])\n","        perm_preds = clf.predict(X[test_idx])\n","        perm_fold_accs.append(accuracy_score(y_perm[test_idx], perm_preds))\n","    perm_accs.append(np.mean(perm_fold_accs))\n","\n","perm_accs = np.array(perm_accs)\n","p_val_class = np.mean(perm_accs >= mean_acc)\n","print(\"Permutation p-value for classifier:\", p_val_class)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"onlEBlG2sfEx","executionInfo":{"status":"error","timestamp":1757371767886,"user_tz":420,"elapsed":56239,"user":{"displayName":"Davidson Lab","userId":"15145557194024733869"}},"outputId":"62f02a6c-a531-40b6-8df9-201abea0aae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean correlation - responders: 0.5222061973246399\n","Mean correlation - non-responders: 0.4945713722099749\n","Mean correlation between groups: 0.5052354151151565\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1147584920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobserved_within\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mobs_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperm_test_corr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Observed within-group difference:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Permutation p-value (within-group correlation difference):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val_corr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1147584920.py\u001b[0m in \u001b[0;36mperm_test_corr\u001b[0;34m(matrix, outcomes, n_permutations)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mperm_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         perm_diff = (\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mmean_pairwise_corr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm_labels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mmean_pairwise_corr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm_labels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         )\n","\u001b[0;32m/tmp/ipython-input-1147584920.py\u001b[0m in \u001b[0;36mmean_pairwise_corr\u001b[0;34m(matrix, indices)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcorr_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[1;32m   2912\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   2913\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0maweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2749\u001b[0;31m     \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2750\u001b[0m     \u001b[0mw_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mavg_as_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_as_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def inter_mixup(X_train, y_train, n_samples, alpha=0.2, hard_labels=False):\n","    \"\"\"\n","    Generates n_samples synthetic examples using mixup.\n","    \"\"\"\n","    X_new = []\n","    y_new = []\n","\n","    for _ in range(n_samples):\n","        i, j = np.random.choice(len(X_train), size=2, replace=False)\n","        lam = np.random.beta(alpha, alpha)\n","        x_mix = lam * X_train[i] + (1 - lam) * X_train[j]\n","        y_mix = lam * y_train[i] + (1 - lam) * y_train[j]\n","\n","        if hard_labels:\n","            y_mix = int(round(y_mix))  # convert to 0 or 1\n","\n","        X_new.append(x_mix)\n","        y_new.append(y_mix)\n","\n","    return np.vstack(X_new), np.array(y_new)\n","\n","\n","def intra_mixup(X_train, y_train, n_samples_per_class, alpha=0.2):\n","    \"\"\"\n","    Generates synthetic samples by mixing features within the same class.\n","    X_train: (n_samples, n_features)\n","    y_train: (n_samples,)\n","    n_samples_per_class: number of synthetic samples to generate per class\n","    alpha: Beta distribution parameter\n","    \"\"\"\n","    X_new, y_new = [], []\n","    classes = np.unique(y_train)\n","\n","    for c in classes:\n","        idx_class = np.where(y_train == c)[0]\n","        for _ in range(n_samples_per_class):\n","            i, j = np.random.choice(idx_class, size=2, replace=False)\n","            lam = np.random.beta(alpha, alpha)\n","            x_mix = lam * X_train[i] + (1 - lam) * X_train[j]\n","            X_new.append(x_mix)\n","            y_new.append(c)  # hard label\n","\n","    return np.vstack(X_new), np.array(y_new)\n","\n","\n","import networkx as nx\n","import numpy as np\n","\n","def compute_more_graph_features(adj_matrix):\n","    \"\"\"\n","    Computes a variety of node- and graph-level features for a weighted adjacency matrix.\n","    Returns a 1D feature vector.\n","    \"\"\"\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","\n","    # ===========================\n","    # Node-level metrics (mean + std)\n","    # ===========================\n","    # Node strength (weighted degree)\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    features.extend([strength.mean(), strength.std()])\n","\n","    # Clustering coefficient\n","    clustering = np.array(list(nx.clustering(G, weight='weight').values()))\n","    features.extend([clustering.mean(), clustering.std()])\n","\n","    # Betweenness centrality\n","    try:\n","        bc = np.array(list(nx.betweenness_centrality(G, weight='weight').values()))\n","        features.extend([bc.mean(), bc.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # Eigenvector centrality\n","    try:\n","        ec = np.array(list(nx.eigenvector_centrality(G, weight='weight', max_iter=500).values()))\n","        features.extend([ec.mean(), ec.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # PageRank\n","    try:\n","        pr = np.array(list(nx.pagerank(G, weight='weight').values()))\n","        features.extend([pr.mean(), pr.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # ===========================\n","    # Global graph metrics\n","    # ===========================\n","    # Global efficiency\n","    try:\n","        features.append(nx.global_efficiency(G))\n","    except:\n","        features.append(np.nan)\n","\n","    # Average clustering\n","    try:\n","        features.append(nx.average_clustering(G, weight='weight'))\n","    except:\n","        features.append(np.nan)\n","\n","    # Transitivity\n","    try:\n","        features.append(nx.transitivity(G))\n","    except:\n","        features.append(np.nan)\n","\n","    # Assortativity (degree)\n","    try:\n","        features.append(nx.degree_assortativity_coefficient(G, weight='weight'))\n","    except:\n","        features.append(np.nan)\n","\n","    # Density\n","    try:\n","        features.append(nx.density(G))\n","    except:\n","        features.append(np.nan)\n","\n","    return np.array(features)\n","\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","print(graphs_by_sid['epp270']['abs'])\n","print(graphs_by_sid['epp270']['rel'])"],"metadata":{"id":"o0Wx6JI40in0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757528722929,"user_tz":420,"elapsed":6327,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"0f795a50-2c0c-4d3a-c43a-5507a7ace3fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.17338755 0.12403889 0.58831806 0.23387851 0.04568071\n","  0.33025175 0.0296529  0.10872398 0.07329236 0.10732256 0.17219024\n","  0.09179144 0.09451418 0.21203314]\n"," [0.17338755 0.         0.63607592 0.0721911  0.51286114 0.39463516\n","  0.31410516 0.23677453 0.12476689 0.01937445 0.13113582 0.09955679\n","  0.28756996 0.09870434 0.02389843]\n"," [0.12403889 0.63607592 0.         0.09948099 0.31710779 0.58452413\n","  0.06856401 0.12770892 0.27110495 0.27026323 0.14050177 0.39097136\n","  0.15021865 0.30674262 0.06529333]\n"," [0.58831806 0.0721911  0.09948099 0.         0.20853883 0.24963951\n","  0.29828674 0.48810218 0.10053241 0.21720226 0.20801852 0.19070761\n","  0.12809837 0.17672336 0.50478525]\n"," [0.23387851 0.51286114 0.31710779 0.20853883 0.         0.00130209\n","  0.26577205 0.26334115 0.14032486 0.23866158 0.16743091 0.34795873\n","  0.23701888 0.11188243 0.04141316]\n"," [0.04568071 0.39463516 0.58452413 0.24963951 0.00130209 0.\n","  0.00597454 0.08512837 0.35144027 0.32152661 0.27190813 0.048595\n","  0.01981576 0.03115809 0.27615433]\n"," [0.33025175 0.31410516 0.06856401 0.29828674 0.26577205 0.00597454\n","  0.         0.02672582 0.23596883 0.07442844 0.26293281 0.24194428\n","  0.33678252 0.30012641 0.13759314]\n"," [0.0296529  0.23677453 0.12770892 0.48810218 0.26334115 0.08512837\n","  0.02672582 0.         0.67013567 0.73429166 0.926853   0.17169178\n","  0.36406677 0.19121499 0.74381622]\n"," [0.10872398 0.12476689 0.27110495 0.10053241 0.14032486 0.35144027\n","  0.23596883 0.67013567 0.         0.76566125 0.76102201 0.22726389\n","  0.23260459 0.1130046  0.51504866]\n"," [0.07329236 0.01937445 0.27026323 0.21720226 0.23866158 0.32152661\n","  0.07442844 0.73429166 0.76566125 0.         0.66895289 0.15973777\n","  0.44840262 0.27026612 0.59321608]\n"," [0.10732256 0.13113582 0.14050177 0.20801852 0.16743091 0.27190813\n","  0.26293281 0.926853   0.76102201 0.66895289 0.         0.04765426\n","  0.15972329 0.13117221 0.3613075 ]\n"," [0.17219024 0.09955679 0.39097136 0.19070761 0.34795873 0.048595\n","  0.24194428 0.17169178 0.22726389 0.15973777 0.04765426 0.\n","  0.24789284 0.830729   0.15755834]\n"," [0.09179144 0.28756996 0.15021865 0.12809837 0.23701888 0.01981576\n","  0.33678252 0.36406677 0.23260459 0.44840262 0.15972329 0.24789284\n","  0.         0.00951284 0.4519716 ]\n"," [0.09451418 0.09870434 0.30674262 0.17672336 0.11188243 0.03115809\n","  0.30012641 0.19121499 0.1130046  0.27026612 0.13117221 0.830729\n","  0.00951284 0.         0.25332859]\n"," [0.21203314 0.02389843 0.06529333 0.50478525 0.04141316 0.27615433\n","  0.13759314 0.74381622 0.51504866 0.59321608 0.3613075  0.15755834\n","  0.4519716  0.25332859 0.        ]]\n","[[ 0.          0.17338755  0.12403889  0.58831806 -0.23387851  0.04568071\n","   0.33025175 -0.0296529   0.10872398 -0.07329236  0.10732256 -0.17219024\n","  -0.09179144 -0.09451418  0.21203314]\n"," [ 0.17338755  0.          0.63607592  0.0721911   0.51286114  0.39463516\n","  -0.31410515  0.23677453 -0.12476689  0.01937445  0.13113582 -0.09955679\n","  -0.28756996  0.09870434 -0.02389843]\n"," [ 0.12403889  0.63607592  0.          0.09948099  0.31710779  0.58452413\n","  -0.06856401  0.12770892 -0.27110495 -0.27026323 -0.14050177 -0.39097136\n","  -0.15021865 -0.30674262 -0.06529333]\n"," [ 0.58831806  0.0721911   0.09948099  0.         -0.20853883  0.24963951\n","   0.29828674 -0.48810218 -0.10053241 -0.21720226 -0.20801852 -0.19070761\n","   0.12809837 -0.17672336  0.50478525]\n"," [-0.23387851  0.51286114  0.31710779 -0.20853883  0.          0.00130209\n","  -0.26577205  0.26334115 -0.14032486  0.23866158  0.16743091 -0.34795873\n","  -0.23701888 -0.11188243 -0.04141316]\n"," [ 0.04568071  0.39463516  0.58452413  0.24963951  0.00130209  0.\n","  -0.00597454 -0.08512837 -0.35144027 -0.32152661 -0.27190813 -0.048595\n","  -0.01981576  0.03115809  0.27615433]\n"," [ 0.33025175 -0.31410515 -0.06856401  0.29828674 -0.26577205 -0.00597454\n","   0.          0.02672582  0.23596883 -0.07442844  0.26293281 -0.24194428\n","   0.33678252 -0.30012641  0.13759314]\n"," [-0.0296529   0.23677453  0.12770892 -0.48810218  0.26334115 -0.08512837\n","   0.02672582  0.          0.67013567  0.73429166  0.926853    0.17169178\n","  -0.36406677  0.19121499 -0.74381622]\n"," [ 0.10872398 -0.12476689 -0.27110495 -0.10053241 -0.14032486 -0.35144027\n","   0.23596883  0.67013567  0.          0.76566125  0.76102201  0.22726389\n","  -0.23260459  0.1130046  -0.51504866]\n"," [-0.07329236  0.01937445 -0.27026323 -0.21720226  0.23866158 -0.32152661\n","  -0.07442844  0.73429166  0.76566125  0.          0.66895289  0.15973777\n","  -0.44840262  0.27026612 -0.59321608]\n"," [ 0.10732256  0.13113582 -0.14050177 -0.20801852  0.16743091 -0.27190813\n","   0.26293281  0.926853    0.76102201  0.66895289  0.          0.04765426\n","  -0.15972329  0.13117221 -0.3613075 ]\n"," [-0.17219024 -0.09955679 -0.39097136 -0.19070761 -0.34795873 -0.048595\n","  -0.24194428  0.17169178  0.22726389  0.15973777  0.04765426  0.\n","   0.24789284  0.830729   -0.15755834]\n"," [-0.09179144 -0.28756996 -0.15021865  0.12809837 -0.23701888 -0.01981576\n","   0.33678252 -0.36406677 -0.23260459 -0.44840262 -0.15972329  0.24789284\n","   0.          0.00951284  0.4519716 ]\n"," [-0.09451418  0.09870434 -0.30674262 -0.17672336 -0.11188243  0.03115809\n","  -0.30012641  0.19121499  0.1130046   0.27026612  0.13117221  0.830729\n","   0.00951284  0.         -0.25332859]\n"," [ 0.21203314 -0.02389843 -0.06529333  0.50478525 -0.04141316  0.27615433\n","   0.13759314 -0.74381622 -0.51504866 -0.59321608 -0.3613075  -0.15755834\n","   0.4519716  -0.25332859  0.        ]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import networkx as nx\n","\n","\n","from sklearn.neural_network import MLPRegressor\n","\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score, confusion_matrix, roc_curve\n",")\n","\n","def add_noise(X, sigma=0.01):\n","    return X + np.random.normal(0, sigma, X.shape)\n","\n","# ===========================\n","# 1. Graph feature computation\n","# ===========================\n","def compute_graph_features(adj_matrix, feature_flags):\n","    \"\"\"\n","    Computes node- and graph-level features for a weighted adjacency matrix.\n","    Returns a 1D feature vector.\n","    \"\"\"\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","\n","    # Node-level metrics\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    clustering_dict = nx.clustering(G, weight='weight')\n","    clustering = np.array(list(clustering_dict.values()))\n","\n","    if feature_flags.get(\"strength_mean\", True):\n","        features.append(strength.mean())\n","    if feature_flags.get(\"strength_std\", True):\n","        features.append(strength.std())\n","    if feature_flags.get(\"clustering_mean\", True):\n","        features.append(clustering.mean())\n","    if feature_flags.get(\"clustering_std\", True):\n","        features.append(clustering.std())\n","    if feature_flags.get(\"avg_clustering\", True):\n","        try:\n","            features.append(nx.average_clustering(G, weight='weight'))\n","        except:\n","            features.append(np.nan)\n","\n","    return np.array(features)\n","\n","def threshold_graph(A, keep_fraction):\n","    triu_indices = np.triu_indices_from(A, k=1)\n","    edge_values = A[triu_indices]\n","    edge_values = edge_values[edge_values > 0]  # ignore zeros\n","    if len(edge_values) == 0:\n","        return np.zeros_like(A)\n","    threshold = np.percentile(edge_values, 100 * (1 - keep_fraction))\n","    mask = A >= threshold\n","    A_thresh = A * mask\n","    A_thresh = np.maximum(A_thresh, A_thresh.T)  # enforce symmetry\n","    return A_thresh\n","\n","# ===========================\n","# 2. Load data and build feature matrix\n","# ===========================\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","\n","\n","\n","# ---------------------------\n","# Feature flags (toggle True/False)\n","# ---------------------------\n","feature_flags = {\n","    \"strength_mean\": True,\n","    \"strength_std\": True,\n","    \"clustering_mean\": True,\n","    \"clustering_std\": True,\n","    \"avg_clustering\": True\n","}\n","\n","feature_list = []\n","for sid in graphs_by_sid.keys():\n","    adj = graphs_by_sid[sid]['abs']  # use 'abs' or 'rel'\n","    #features = compute_more_graph_features(adj)\n","    #adj = threshold_graph(adj, 1.0)\n","    features = compute_graph_features(adj, feature_flags)\n","    feature_list.append(features)\n","\n","X = np.array(feature_list)\n","y = np.array([graphs_by_sid[sid]['outcome'] for sid in graphs_by_sid.keys()])\n","\n","# Remove NaN columns\n","nan_cols = np.isnan(X).any(axis=0)\n","X = X[:, ~nan_cols]\n","print(\"X shape after NaN removal:\", X.shape)\n","print(\"feature_flags:\", feature_flags)\n","\n","print((y == 1).sum())  # number of elements equal to 1\n","print((y == 0).sum())  # number of elements equal to 0\n","\n","# Dynamically generate feature names based on flags and non-NaN\n","feature_names = [name for name, include in feature_flags.items() if include]\n","\n","# ===========================\n","# 3. 20-run 5-fold CV\n","# ===========================\n","clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n","\n","n_runs = 20  # number of repeated CV runs\n","\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","all_coefs = []\n","\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + run)\n","\n","    accs, precisions, recalls, f1s, aucs, coefs = [], [], [], [], [], []\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","\n","        # ---------------------------\n","        # Mixup (optional)\n","        # ---------------------------\n","        X_aug, y_aug = intra_mixup(X_train, y_train, n_samples_per_class=40) #same class mixup\n","        X_train_aug = np.vstack([X_train, X_aug])\n","        y_train_aug = np.concatenate([y_train, y_aug])\n","\n","        X_train_aug, y_train_aug = X_train, y_train  # no mixup\n","        X_train_aug = add_noise(X_train_aug)\n","\n","        clf.fit(X_train_aug, y_train_aug)\n","\n","        y_pred = clf.predict(X[test_idx])\n","        y_prob = clf.predict_proba(X[test_idx])[:,1]\n","\n","        accs.append(accuracy_score(y[test_idx], y_pred))\n","        precisions.append(precision_score(y[test_idx], y_pred))\n","        recalls.append(recall_score(y[test_idx], y_pred))\n","        f1s.append(f1_score(y[test_idx], y_pred))\n","        aucs.append(roc_auc_score(y[test_idx], y_prob))\n","\n","        log_reg = clf.named_steps['logisticregression']\n","        coefs.append(log_reg.coef_[0])\n","\n","    # Append this run's results to overall\n","    all_accs.extend(accs)\n","    all_precisions.extend(precisions)\n","    all_recalls.extend(recalls)\n","    all_f1s.extend(f1s)\n","    all_aucs.extend(aucs)\n","    all_coefs.append(np.mean(coefs, axis=0))\n","\n","# ===========================\n","# 4. Report overall metrics\n","# ===========================\n","print(\"\\n=== Overall 20-run 5-fold CV metrics ===\")\n","print(f\"Accuracy  : {np.mean(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(all_aucs):.3f}\")\n","\n","# ===========================\n","# 5. Feature importance\n","# ===========================\n","mean_coef = np.mean(all_coefs, axis=0)\n","importance = np.abs(mean_coef)\n","\n","print(\"\\nFeature importance (avg |coef| across runs):\")\n","for name, imp in zip(feature_names, importance):\n","    print(f\"{name:<20}: {imp:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rFpZ0G0xwrG","executionInfo":{"status":"ok","timestamp":1757530255798,"user_tz":420,"elapsed":8129,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"8e6aaa37-f15c-4673-a6d3-c52549715008"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["X shape after NaN removal: (56, 5)\n","feature_flags: {'strength_mean': True, 'strength_std': True, 'clustering_mean': True, 'clustering_std': True, 'avg_clustering': True}\n","29\n","27\n","=== Run 1/20 ===\n","=== Run 2/20 ===\n","=== Run 3/20 ===\n","=== Run 4/20 ===\n","=== Run 5/20 ===\n","=== Run 6/20 ===\n","=== Run 7/20 ===\n","=== Run 8/20 ===\n","=== Run 9/20 ===\n","=== Run 10/20 ===\n","=== Run 11/20 ===\n","=== Run 12/20 ===\n","=== Run 13/20 ===\n","=== Run 14/20 ===\n","=== Run 15/20 ===\n","=== Run 16/20 ===\n","=== Run 17/20 ===\n","=== Run 18/20 ===\n","=== Run 19/20 ===\n","=== Run 20/20 ===\n","\n","=== Overall 20-run 5-fold CV metrics ===\n","Accuracy  : 0.635\n","Precision : 0.654\n","Recall    : 0.658\n","F1-score  : 0.639\n","AUC       : 0.713\n","\n","Feature importance (avg |coef| across runs):\n","strength_mean       : 0.368\n","strength_std        : 0.448\n","clustering_mean     : 0.181\n","clustering_std      : 0.175\n","avg_clustering      : 0.308\n"]}]},{"cell_type":"code","source":["#inter class mixup\n","\n","from sklearn.linear_model import SGDRegressor\n","\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + run)\n","\n","    accs, precisions, recalls, f1s, aucs = [], [], [], [], []\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","\n","        # ---------------------------\n","        # Mixup: produce soft labels\n","        # ---------------------------\n","        X_aug, y_aug = inter_mixup(X_train, y_train, n_samples=40)\n","        X_train_aug = np.vstack([X_train, X_aug])\n","        y_train_aug = np.concatenate([y_train, y_aug])\n","\n","        # Add small noise\n","        X_train_aug = add_noise(X_train_aug)\n","\n","        # ---------------------------\n","        # Fit regression model\n","        # ---------------------------\n","        clf = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3))\n","        clf.fit(X_train_aug, y_train_aug)\n","\n","        # Predict probabilities on test set\n","        y_prob = clf.predict(X[test_idx])\n","        y_prob = np.clip(y_prob, 0, 1)  # ensure in [0,1]\n","\n","        # Threshold for discrete labels\n","        y_pred = (y_prob >= 0.5).astype(int)\n","\n","        # Compute metrics\n","        accs.append(accuracy_score(y[test_idx], y_pred))\n","        precisions.append(precision_score(y[test_idx], y_pred))\n","        recalls.append(recall_score(y[test_idx], y_pred))\n","        f1s.append(f1_score(y[test_idx], y_pred))\n","        aucs.append(roc_auc_score(y[test_idx], y_prob))\n","\n","    # Append this runs results\n","    all_accs.extend(accs)\n","    all_precisions.extend(precisions)\n","    all_recalls.extend(recalls)\n","    all_f1s.extend(f1s)\n","    all_aucs.extend(aucs)\n","\n","# ---------------------------\n","# Report overall metrics\n","# ---------------------------\n","print(\"\\n=== Overall 20-run 5-fold CV metrics ===\")\n","print(f\"Accuracy  : {np.mean(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(all_aucs):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtDYhDN2uZ01","executionInfo":{"status":"ok","timestamp":1757274549843,"user_tz":420,"elapsed":2052,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"181d4d08-4e4d-4d1b-b9b8-e4860d7a2205"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Run 1/20 ===\n","=== Run 2/20 ===\n","=== Run 3/20 ===\n","=== Run 4/20 ===\n","=== Run 5/20 ===\n","=== Run 6/20 ===\n","=== Run 7/20 ===\n","=== Run 8/20 ===\n","=== Run 9/20 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["=== Run 10/20 ===\n","=== Run 11/20 ===\n","=== Run 12/20 ===\n","=== Run 13/20 ===\n","=== Run 14/20 ===\n","=== Run 15/20 ===\n","=== Run 16/20 ===\n","=== Run 17/20 ===\n","=== Run 18/20 ===\n","=== Run 19/20 ===\n","=== Run 20/20 ===\n","\n","=== Overall 20-run 5-fold CV metrics ===\n","Accuracy  : 0.639\n","Precision : 0.669\n","Recall    : 0.595\n","F1-score  : 0.610\n","AUC       : 0.734\n"]}]},{"cell_type":"code","source":["#random forest\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import networkx as nx\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score\n",")\n","\n","# ===========================\n","# 1. Graph feature computation\n","# ===========================\n","def compute_graph_features(adj_matrix):\n","    \"\"\"\n","    Computes node- and graph-level features for a weighted adjacency matrix.\n","    Returns a 1D feature vector.\n","    \"\"\"\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    clustering_dict = nx.clustering(G, weight='weight')\n","    clustering = np.array(list(clustering_dict.values()))\n","\n","    # append features one by one\n","    features.append(strength.mean())\n","    features.append(strength.std())\n","    features.append(clustering.mean())\n","    features.append(clustering.std())\n","    features.append(nx.average_clustering(G, weight='weight'))\n","\n","    return np.array(features)\n","\n","# ===========================\n","# 2. Load data and build feature matrix\n","# ===========================\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","\n","feature_list = []\n","for sid in graphs_by_sid.keys():\n","    adj = graphs_by_sid[sid]['abs']\n","    features = compute_graph_features(adj)\n","    feature_list.append(features)\n","\n","X = np.array(feature_list)\n","y = np.array([graphs_by_sid[sid]['outcome'] for sid in graphs_by_sid.keys()])\n","\n","# Remove NaN columns\n","nan_cols = np.isnan(X).any(axis=0)\n","X = X[:, ~nan_cols]\n","\n","# Set feature names for included columns\n","feature_flags = {\n","    \"strength_mean\": True,\n","    \"strength_std\": True,\n","    \"clustering_mean\": True,\n","    \"clustering_std\": True,\n","    \"avg_clustering\": True\n","}\n","feature_names = [name for name, include in feature_flags.items() if include]\n","\n","# ===========================\n","# 3. Cross-validated RF classification\n","# ===========================\n","n_runs = 20\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","all_importances = []\n","\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + run)\n","\n","    # Temporary lists for this run\n","    accs, precisions, recalls, f1s, aucs, importances = [], [], [], [], [], []\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","        X_test, y_test = X[test_idx], y[test_idx]\n","\n","        # Random Forest classifier\n","        clf = RandomForestClassifier(n_estimators=200, random_state=42)\n","        clf.fit(X_train, y_train)\n","\n","        y_pred = clf.predict(X_test)\n","        y_prob = clf.predict_proba(X_test)[:, 1]\n","\n","        accs.append(accuracy_score(y_test, y_pred))\n","        precisions.append(precision_score(y_test, y_pred))\n","        recalls.append(recall_score(y_test, y_pred))\n","        f1s.append(f1_score(y_test, y_pred))\n","        aucs.append(roc_auc_score(y_test, y_prob))\n","        importances.append(clf.feature_importances_)\n","\n","    all_accs.extend(accs)\n","    all_precisions.extend(precisions)\n","    all_recalls.extend(recalls)\n","    all_f1s.extend(f1s)\n","    all_aucs.extend(aucs)\n","    all_importances.append(np.mean(importances, axis=0))\n","\n","# ===========================\n","# 4. Overall metrics and feature importances\n","# ===========================\n","print(\"\\n=== Overall 20-run 5-fold CV metrics ===\")\n","print(f\"Accuracy  : {np.mean(all_accs):.3f}  {np.std(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(all_precisions):.3f}  {np.std(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(all_recalls):.3f}  {np.std(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(all_f1s):.3f}  {np.std(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(all_aucs):.3f}  {np.std(all_aucs):.3f}\")\n","\n","# Mean feature importance across runs\n","mean_importance = np.mean(all_importances, axis=0)\n","print(\"\\nFeature importance (avg across runs):\")\n","for name, imp in sorted(zip(feature_names, mean_importance), key=lambda x: -x[1]):\n","    print(f\"{name:<15}: {imp:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXXKdvWrafYC","executionInfo":{"status":"ok","timestamp":1756004340507,"user_tz":420,"elapsed":47631,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"c2216275-8548-4d1e-cf69-cc1243da4b57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Run 1/20 ===\n","=== Run 2/20 ===\n","=== Run 3/20 ===\n","=== Run 4/20 ===\n","=== Run 5/20 ===\n","=== Run 6/20 ===\n","=== Run 7/20 ===\n","=== Run 8/20 ===\n","=== Run 9/20 ===\n","=== Run 10/20 ===\n","=== Run 11/20 ===\n","=== Run 12/20 ===\n","=== Run 13/20 ===\n","=== Run 14/20 ===\n","=== Run 15/20 ===\n","=== Run 16/20 ===\n","=== Run 17/20 ===\n","=== Run 18/20 ===\n","=== Run 19/20 ===\n","=== Run 20/20 ===\n","\n","=== Overall 20-run 5-fold CV metrics ===\n","Accuracy  : 0.637  0.123\n","Precision : 0.666  0.173\n","Recall    : 0.608  0.180\n","F1-score  : 0.624  0.154\n","AUC       : 0.653  0.142\n","\n","Feature importance (avg across runs):\n","strength_std   : 0.229\n","strength_mean  : 0.227\n","clustering_std : 0.206\n","avg_clustering : 0.184\n","clustering_mean: 0.155\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import networkx as nx\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score\n",")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# ===========================\n","# 1. Graph feature computation\n","# ===========================\n","def compute_graph_features(adj_matrix):\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    clustering_dict = nx.clustering(G, weight='weight')\n","    clustering = np.array(list(clustering_dict.values()))\n","\n","    features.append(strength.mean())\n","    features.append(clustering.mean())\n","    features.append(clustering.std())\n","\n","    return np.array(features)\n","\n","# ===========================\n","# 2. Load data and build feature matrix\n","# ===========================\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","\n","feature_list = []\n","for sid in graphs_by_sid.keys():\n","    adj = graphs_by_sid[sid]['abs']\n","    features = compute_graph_features(adj)\n","    feature_list.append(features)\n","\n","X = np.array(feature_list)\n","y = np.array([graphs_by_sid[sid]['outcome'] for sid in graphs_by_sid.keys()])\n","\n","# Remove NaNs\n","nan_cols = np.isnan(X).any(axis=0)\n","X = X[:, ~nan_cols]\n","\n","feature_names = [\"strength_mean\", \"clustering_mean\", \"clustering_std\"]\n","X = StandardScaler().fit_transform(X)\n","\n","# ===========================\n","# 3. Neural network definition\n","# ===========================\n","class SmallNN(nn.Module):\n","    def __init__(self, input_dim):\n","        super(SmallNN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 8)\n","        self.fc2 = nn.Linear(8, 4)\n","        self.out = nn.Linear(4, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = torch.sigmoid(self.out(x))\n","        return x\n","\n","# ===========================\n","# 4. Mixup function\n","# ===========================\n","def mixup_data(X, y, alpha=0.2):\n","    lam = np.random.beta(alpha, alpha)\n","    batch_size = X.shape[0]\n","    index = np.random.permutation(batch_size)\n","    X_mix = lam * X + (1 - lam) * X[index]\n","    y_mix = lam * y + (1 - lam) * y[index]\n","    return X_mix, y_mix\n","\n","# ===========================\n","# 5. 20-run 5-fold CV\n","# ===========================\n","n_runs = 20\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42+run)\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","        X_test, y_test = X[test_idx], y[test_idx]\n","\n","\n","\n","        # Convert to torch tensors\n","        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n","        y_train_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n","        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n","        y_test_t = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n","\n","        # Apply mixup\n","        X_train_t, y_train_t = mixup_data(X_train_t.numpy(), y_train_t.numpy())\n","        X_train_t = torch.tensor(X_train_t, dtype=torch.float32)\n","        y_train_t = torch.tensor(y_train_t, dtype=torch.float32)\n","\n","        # Dataset and loader\n","        train_dataset = TensorDataset(X_train_t, y_train_t)\n","        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","        # Model\n","        model = SmallNN(input_dim=X.shape[1])\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","        criterion = nn.BCELoss()\n","\n","        # Train\n","        model.train()\n","        for epoch in range(100):\n","            for xb, yb in train_loader:\n","                optimizer.zero_grad()\n","                y_pred = model(xb)\n","                loss = criterion(y_pred, yb)\n","                loss.backward()\n","                optimizer.step()\n","\n","        # Evaluate\n","        model.eval()\n","        with torch.no_grad():\n","            y_pred_prob = model(X_test_t).numpy().flatten()\n","            y_pred_label = (y_pred_prob >= 0.5).astype(int)\n","\n","        all_accs.append(accuracy_score(y_test, y_pred_label))\n","        all_precisions.append(precision_score(y_test, y_pred_label))\n","        all_recalls.append(recall_score(y_test, y_pred_label))\n","        all_f1s.append(f1_score(y_test, y_pred_label))\n","        all_aucs.append(roc_auc_score(y_test, y_pred_prob))\n","\n","# ===========================\n","# 6. Overall metrics\n","# ===========================\n","print(\"\\n=== Overall 20-run 5-fold CV metrics (NN) ===\")\n","print(f\"Accuracy  : {np.mean(all_accs):.3f}  {np.std(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(all_precisions):.3f}  {np.std(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(all_recalls):.3f}  {np.std(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(all_f1s):.3f}  {np.std(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(all_aucs):.3f}  {np.std(all_aucs):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEfel5P3hJlP","executionInfo":{"status":"ok","timestamp":1757027752643,"user_tz":420,"elapsed":45283,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"6c8110d9-2026-4d53-f023-d076b9494ee8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Run 1/20 ===\n","=== Run 2/20 ===\n","=== Run 3/20 ===\n","=== Run 4/20 ===\n","=== Run 5/20 ===\n","=== Run 6/20 ===\n","=== Run 7/20 ===\n","=== Run 8/20 ===\n","=== Run 9/20 ===\n","=== Run 10/20 ===\n","=== Run 11/20 ===\n","=== Run 12/20 ===\n","=== Run 13/20 ===\n","=== Run 14/20 ===\n","=== Run 15/20 ===\n","=== Run 16/20 ===\n","=== Run 17/20 ===\n","=== Run 18/20 ===\n","=== Run 19/20 ===\n","=== Run 20/20 ===\n","\n","=== Overall 20-run 5-fold CV metrics (NN) ===\n","Accuracy  : 0.596  0.132\n","Precision : 0.622  0.153\n","Recall    : 0.601  0.208\n","F1-score  : 0.593  0.153\n","AUC       : 0.656  0.149\n"]}]},{"cell_type":"code","source":["# Logistic regression intra mixup  (same class mixup)\n","# === Overall 20-run 5-fold CV metrics ===\n","# Accuracy  : 0.620\n","# Precision : 0.649\n","# Recall    : 0.605\n","# F1-score  : 0.610\n","# AUC       : 0.727\n","\n","# Linear regression inter mixup (different class mixup)\n","# === Overall 20-run 5-fold CV metrics ===\n","# Accuracy  : 0.639\n","# Precision : 0.669\n","# Recall    : 0.595\n","# F1-score  : 0.610\n","# AUC       : 0.734\n","\n","# Logistic regression no mixup\n","# === Overall 20-run 5-fold CV metrics ===\n","# Accuracy  : 0.634\n","# Precision : 0.657\n","# Recall    : 0.647\n","# F1-score  : 0.636\n","# AUC       : 0.740"],"metadata":{"id":"7IQBfzSJhmlx"},"execution_count":null,"outputs":[]}]}