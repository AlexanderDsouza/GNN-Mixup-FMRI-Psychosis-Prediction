{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19w5okz3QHV9XusGr1ZywaLLmeg3G6-pd","timestamp":1757371573081}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Core libraries\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","from scipy.linalg import eigh\n","\n","# ===========================\n","# Load and preprocess dataset\n","# ===========================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjLFZcSjxBVr","executionInfo":{"status":"ok","timestamp":1757817348984,"user_tz":420,"elapsed":26371,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"outputId":"ffcd31c3-4e7d-463f-8f7c-8f1397609698"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkSE2CWmwZU7"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def load_schaefer_graphs(\n","    keep_fraction=0.3,\n","    outcome_col='Imp20PercentBPRS'\n","):\n","    # ===========================\n","    # Hardcoded file names\n","    # ===========================\n","    graph_file = \"/content/drive/Shared drives/GNN/SchaeferAtlas_Rest_Results_Freq_008to09_wholebrain.xlsx\"\n","    outcome_file = \"/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx\"\n","\n","    demo_cols = ['Age', 'handedness', 'sex', 'PrimaryEthnicity',\n","                 'PrimaryRace', 'Education', 'Parental Education']\n","\n","    # ===========================\n","    # Load outcome + demographics\n","    # ===========================\n","    df_labels = pd.read_excel(outcome_file, sheet_name='outcomeanddemographics', skiprows=1)\n","    demo_dict = df_labels.set_index('SID')[demo_cols].to_dict(orient='index')\n","\n","    # Helper: threshold adjacency\n","    def threshold_graph(A, keep_fraction):\n","        triu_indices = np.triu_indices_from(A, k=1)\n","        edge_values = A[triu_indices]\n","        edge_values = edge_values[edge_values > 0]  # ignore zeros\n","        if len(edge_values) == 0:\n","            return np.zeros_like(A)\n","        threshold = np.percentile(edge_values, 100 * (1 - keep_fraction))\n","        mask = A >= threshold\n","        A_thresh = A * mask\n","        A_thresh = np.maximum(A_thresh, A_thresh.T)  # enforce symmetry\n","        return A_thresh\n","\n","    # ===========================\n","    # Load only needed sheets\n","    # ===========================\n","    xl = pd.ExcelFile(graph_file)\n","    available_sheets = set(xl.sheet_names)\n","\n","    # Get SIDs that actually have outcomes\n","    sids_with_outcome = df_labels['SID'].dropna().unique()\n","    sids_to_load = [sid for sid in sids_with_outcome if sid in available_sheets]\n","\n","    graphs_by_sid = {}\n","\n","    for sid in sids_to_load:\n","        try:\n","            # Load matrix for this patient\n","            mat_df = pd.read_excel(graph_file, sheet_name=sid, index_col=0)\n","            mat = mat_df.values.astype(float)\n","\n","            mat = mat_df.values.astype(float)\n","\n","            # Replace NaNs and infs with 0\n","            mat = np.nan_to_num(mat, nan=0.0, posinf=0.0, neginf=0.0)\n","            rel = mat\n","            # Take absolute value (since matrices can have negatives)\n","            mat = np.abs(mat)\n","\n","            # Take absolute values (since matrices aren't absolute in this dataset)\n","\n","\n","            # Get outcome + demographics\n","            row = df_labels.loc[df_labels['SID'] == sid]\n","            outcome = row[outcome_col].values[0]\n","            demos = demo_dict.get(sid, None)\n","\n","            # Threshold adjacency\n","            mat_thresh = threshold_graph(mat, keep_fraction)\n","            rel_thresh = threshold_graph(rel, keep_fraction)\n","\n","            graphs_by_sid[sid] = {\n","                'abs': mat_thresh,      # absolute-valued adjacency\n","                'rel': rel_thresh,\n","                'demo': demos,\n","                'outcome': outcome,\n","                'nodes': list(mat_df.index)  # preserve brain region labels\n","            }\n","        except Exception as e:\n","            print(f\"Skipping SID {sid} due to error: {e}\")\n","            continue\n","\n","    return graphs_by_sid\n"]},{"cell_type":"code","source":["#combines all the previous dataloading\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","\n","from itertools import product\n","import numpy as np\n","\n","\n","def load_and_preprocess_graphs(\n","    file_path,\n","    abs_keep_fraction=0.3,\n","    rel_keep_fraction=0.3,\n","    demo_cols=None,\n","    outcome_col='Imp20PercentBPRS'\n","):\n","    if demo_cols is None:\n","        demo_cols = ['Age', 'handedness', 'sex', 'PrimaryEthnicity', 'PrimaryRace', 'Education', 'Parental Education']\n","\n","    # ===========================\n","    # Load dataset\n","    # ===========================\n","    df = pd.read_excel(file_path)\n","    df_labels = pd.read_excel(file_path, sheet_name='outcomeanddemographics', skiprows=1)\n","\n","    # ===========================\n","    # Extract nodes and edges\n","    # ===========================\n","    edge_columns = df.columns.drop('SID')\n","\n","    # Get all unique nodes\n","    nodes_set = set()\n","    for col in edge_columns:\n","        raw_node1, raw_node2 = col.split('-')\n","        node1 = raw_node1.replace('ABS_', '')\n","        node2 = raw_node2.replace('ABS_', '')\n","        nodes_set.add(node1)\n","        nodes_set.add(node2)\n","\n","    nodes = sorted(list(nodes_set))\n","    n_nodes = len(nodes)\n","    node_to_idx = {node: i for i, node in enumerate(nodes)}\n","\n","    abs_edge_cols, rel_edge_cols = [], []\n","    abs_edge_to_idx, rel_edge_to_idx = [], []\n","\n","    for col in edge_columns:\n","        raw_node1, raw_node2 = col.split('-')\n","        node1 = raw_node1.replace('ABS_', '')\n","        node2 = raw_node2.replace('ABS_', '')\n","        idx1, idx2 = node_to_idx[node1], node_to_idx[node2]\n","\n","        if 'ABS_' in col:\n","            abs_edge_cols.append(col)\n","            abs_edge_to_idx.append((idx1, idx2))\n","        else:\n","            rel_edge_cols.append(col)\n","            rel_edge_to_idx.append((idx1, idx2))\n","\n","    # ===========================\n","    # Build adjacency matrices\n","    # ===========================\n","    abs_adj_matrices, rel_adj_matrices = [], []\n","\n","    for _, row in df.iterrows():\n","        abs_adj = np.zeros((n_nodes, n_nodes))\n","        rel_adj = np.zeros((n_nodes, n_nodes))\n","\n","        for col_idx, (i1, i2) in enumerate(abs_edge_to_idx):\n","            val = row[abs_edge_cols[col_idx]]\n","            abs_adj[i1, i2] = val\n","            abs_adj[i2, i1] = val\n","\n","        for col_idx, (i1, i2) in enumerate(rel_edge_to_idx):\n","            val = row[rel_edge_cols[col_idx]]\n","            rel_adj[i1, i2] = val\n","            rel_adj[i2, i1] = val\n","\n","        abs_adj_matrices.append(abs_adj)\n","        rel_adj_matrices.append(rel_adj)\n","\n","    # ===========================\n","    # Thresholding function\n","    # ===========================\n","    def threshold_graph(A, keep_fraction):\n","        triu_indices = np.triu_indices_from(A, k=1)\n","        edge_values = A[triu_indices]\n","        threshold = np.percentile(edge_values, 100 * (1 - keep_fraction))\n","        mask = A >= threshold\n","        A_thresh = A * mask\n","        A_thresh = np.maximum(A_thresh, A_thresh.T)\n","        return A_thresh\n","\n","    # ===========================\n","    # Build final dictionary\n","    # ===========================\n","    demo_dict = df_labels.set_index('SID')[demo_cols].to_dict(orient='index')\n","\n","    graphs_by_sid = {}\n","    for sid, abs_mat, rel_mat in zip(df['SID'], abs_adj_matrices, rel_adj_matrices):\n","        demos = demo_dict.get(sid, None)\n","        abs_mat_thresh = threshold_graph(abs_mat, abs_keep_fraction)\n","        rel_mat_thresh = threshold_graph(rel_mat, rel_keep_fraction)\n","        outcome = df_labels.loc[df_labels['SID'] == sid, outcome_col].values[0] \\\n","                  if sid in df_labels['SID'].values else None\n","\n","        graphs_by_sid[sid] = {\n","            'abs': abs_mat_thresh,\n","            'rel': rel_mat_thresh,\n","            'demo': demos,\n","            'outcome': outcome\n","        }\n","\n","    # Filter out subjects without outcome\n","    graphs_by_sid = {sid: g for sid, g in graphs_by_sid.items() if g['outcome'] is not None}\n","\n","    return graphs_by_sid\n"],"metadata":{"id":"0_TRRcXOxHQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inter_mixup(X_train, y_train, n_samples, alpha=0.2, hard_labels=False):\n","    \"\"\"\n","    Generates n_samples synthetic examples using mixup.\n","    \"\"\"\n","    X_new = []\n","    y_new = []\n","\n","    for _ in range(n_samples):\n","        i, j = np.random.choice(len(X_train), size=2, replace=False)\n","        lam = np.random.beta(alpha, alpha)\n","        x_mix = lam * X_train[i] + (1 - lam) * X_train[j]\n","        y_mix = lam * y_train[i] + (1 - lam) * y_train[j]\n","\n","        if hard_labels:\n","            y_mix = int(round(y_mix))  # convert to 0 or 1\n","\n","        X_new.append(x_mix)\n","        y_new.append(y_mix)\n","\n","    return np.vstack(X_new), np.array(y_new)\n","\n","\n","def intra_mixup(X_train, y_train, n_samples_per_class, alpha=0.2):\n","    \"\"\"\n","    Generates synthetic samples by mixing features within the same class.\n","    X_train: (n_samples, n_features)\n","    y_train: (n_samples,)\n","    n_samples_per_class: number of synthetic samples to generate per class\n","    alpha: Beta distribution parameter\n","    \"\"\"\n","    X_new, y_new = [], []\n","    classes = np.unique(y_train)\n","\n","    for c in classes:\n","        idx_class = np.where(y_train == c)[0]\n","        for _ in range(n_samples_per_class):\n","            i, j = np.random.choice(idx_class, size=2, replace=False)\n","            lam = np.random.beta(alpha, alpha)\n","            x_mix = lam * X_train[i] + (1 - lam) * X_train[j]\n","            X_new.append(x_mix)\n","            y_new.append(c)  # hard label\n","\n","    return np.vstack(X_new), np.array(y_new)\n","\n","\n","import networkx as nx\n","import numpy as np\n","\n","def compute_more_graph_features(adj_matrix):\n","    \"\"\"\n","    Computes a variety of node- and graph-level features for a weighted adjacency matrix.\n","    Returns a 1D feature vector.\n","    \"\"\"\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","\n","    # ===========================\n","    # Node-level metrics (mean + std)\n","    # ===========================\n","    # Node strength (weighted degree)\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    features.extend([strength.mean(), strength.std()])\n","\n","\n","    # Clustering coefficient\n","    clustering = np.array(list(nx.clustering(G, weight='weight').values()))\n","    features.extend([clustering.mean(), clustering.std()])\n","\n","    # Betweenness centrality\n","    try:\n","        bc = np.array(list(nx.betweenness_centrality(G, weight='weight').values()))\n","        features.extend([bc.mean(), bc.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # Eigenvector centrality\n","    try:\n","        ec = np.array(list(nx.eigenvector_centrality(G, weight='weight', max_iter=500).values()))\n","        features.extend([ec.mean(), ec.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # PageRank\n","    try:\n","        pr = np.array(list(nx.pagerank(G, weight='weight').values()))\n","        features.extend([pr.mean(), pr.std()])\n","    except:\n","        features.extend([np.nan, np.nan])\n","\n","    # ===========================\n","    # Global graph metrics\n","    # ===========================\n","    # Global efficiency\n","    try:\n","        features.append(nx.global_efficiency(G))\n","    except:\n","        features.append(np.nan)\n","\n","    # Average clustering\n","    try:\n","        features.append(nx.average_clustering(G, weight='weight'))\n","    except:\n","        features.append(np.nan)\n","\n","    # Transitivity\n","    try:\n","        features.append(nx.transitivity(G))\n","    except:\n","        features.append(np.nan)\n","\n","    # Assortativity (degree)\n","    try:\n","        features.append(nx.degree_assortativity_coefficient(G, weight='weight'))\n","    except:\n","        features.append(np.nan)\n","\n","    # Density\n","    try:\n","        features.append(nx.density(G))\n","    except:\n","        features.append(np.nan)\n","\n","    return np.array(features)\n"],"metadata":{"id":"o0Wx6JI40in0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import networkx as nx\n","import time\n","\n","\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDRegressor # Corrected import\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    roc_auc_score, confusion_matrix, roc_curve\n",")\n","\n","def add_noise(X, sigma=0.01):\n","    return X + np.random.normal(0, sigma, X.shape)\n","\n","# ===========================\n","# 1. Graph feature computation\n","# ===========================\n","def compute_graph_features(adj_matrix, feature_flags):\n","    \"\"\"\n","    Computes node- and graph-level features for a weighted adjacency matrix.\n","    Returns a 1D feature vector and corresponding feature names.\n","    \"\"\"\n","    G = nx.from_numpy_array(adj_matrix)\n","    features = []\n","    feature_names = []\n","\n","    # Node-level metrics\n","    strength = np.array([s for n, s in G.degree(weight='weight')])\n","    clustering_dict = nx.clustering(G, weight='weight')\n","    clustering = np.array(list(clustering_dict.values()))\n","\n","    if feature_flags.get(\"strength_mean\", True):\n","        features.append(strength.mean())\n","        feature_names.append(\"strength_mean\")\n","    if feature_flags.get(\"strength_std\", True):\n","        features.append(strength.std())\n","        feature_names.append(\"strength_std\")\n","    if feature_flags.get(\"clustering_mean\", True):\n","        features.append(clustering.mean())\n","        feature_names.append(\"clustering_mean\")\n","    if feature_flags.get(\"clustering_std\", True):\n","        features.append(clustering.std())\n","        feature_names.append(\"clustering_std\")\n","\n","    if feature_flags.get(\"betweenness_centrality\", True):\n","        try:\n","            bc = np.array(list(nx.betweenness_centrality(G, weight='weight').values()))\n","            features.extend([bc.mean(), bc.std()])\n","            feature_names.extend([\"betweenness_centrality_mean\", \"betweenness_centrality_std\"])\n","        except:\n","            features.extend([np.nan, np.nan])\n","            feature_names.extend([\"betweenness_centrality_mean\", \"betweenness_centrality_std\"])\n","\n","\n","    if feature_flags.get(\"eigenvector_centrality\", True):\n","        try:\n","            ec = np.array(list(nx.eigenvector_centrality(G, weight='weight', max_iter=500).values()))\n","            features.extend([ec.mean(), ec.std()])\n","            feature_names.extend([\"eigenvector_centrality_mean\", \"eigenvector_centrality_std\"])\n","        except:\n","            features.extend([np.nan, np.nan])\n","            feature_names.extend([\"eigenvector_centrality_mean\", \"eigenvector_centrality_std\"])\n","\n","\n","    if feature_flags.get(\"avg_clustering\", True):\n","        try:\n","            features.append(nx.average_clustering(G, weight='weight'))\n","            feature_names.append(\"avg_clustering\")\n","        except:\n","            features.append(np.nan)\n","            feature_names.append(\"avg_clustering\")\n","\n","    return features, feature_names\n","\n","# ===========================\n","# 2. Load data and build feature matrix\n","# ===========================\n","file_path = '/content/drive/Shared drives/GNN/RestingStateDataforAlex_3Networks.xlsx'\n","graphs_by_sid = load_and_preprocess_graphs(file_path, abs_keep_fraction=1.0, rel_keep_fraction=1.0)\n","graphs_by_sid = load_schaefer_graphs()\n","\n","# ---------------------------\n","# Feature flags (toggle True/False)\n","# ---------------------------\n","feature_flags = {\n","    \"strength_mean\": True,\n","    \"strength_std\": True,\n","    \"clustering_mean\": True,\n","    \"clustering_std\": True,\n","    \"betweenness_centrality\": True, # Added betweenness centrality flag\n","    \"eigenvector_centrality\": True, # Added eigenvector centrality flag\n","    \"avg_clustering\": True\n","}\n","\n","\n","df = pd.read_excel(file_path, sheet_name='outcomeanddemographics', skiprows=1)\n","bprs_map = dict(zip(df['SID'], df['BPRS_Baseline']))\n","print(df.columns)\n","\n","\n","feature_list = []\n","all_feature_names = None\n","for sid in graphs_by_sid.keys():\n","    features, current_feature_names = compute_graph_features(graphs_by_sid[sid]['abs'], feature_flags)\n","\n","    # Convert list to dictionary for easier handling\n","    features_dict = dict(zip(current_feature_names, features))\n","\n","    # Add BPRS_Baseline\n","    features_dict['BPRS_Baseline'] = bprs_map.get(sid, np.nan)\n","\n","    feature_list.append(features_dict)\n","\n","# Convert list of dictionaries to DataFrame\n","all_features_df = pd.DataFrame(feature_list)\n","\n","X = all_features_df.to_numpy()  # shape: (num_sids, num_features)\n","y = np.array([graphs_by_sid[sid]['outcome'] for sid in graphs_by_sid.keys()])\n","\n","# Remove NaN columns and corresponding feature names\n","nan_cols = np.isnan(X).any(axis=0)\n","X = X[:, ~nan_cols]\n","##feature_names = [all_feature_names[i] for i in range(len(all_feature_names)) if not nan_cols[i]]\n","\n","print(\"X shape after NaN removal:\", X.shape)\n","print(\"feature_flags:\", feature_flags)\n","#print(\"feature_names:\", feature_names)\n","\n","\n","print((y == 1).sum())  # number of elements equal to 1\n","print((y == 0).sum())  # number of elements equal to 0\n","\n","\n","# ===========================\n","# 3. 20-run 5-fold CV\n","# ===========================\n","\n","# The prefix B means baseline\n","#clf = make_pipeline(StandardScaler(), SGDRegressor(max_iter=500, tol=1e-3))\n","#Bclf = make_pipeline(StandardScaler(), SGDRegressor(max_iter=500, tol=1e-3))\n","#clf = MLPRegressor(hidden_layer_sizes=(100,100), activation='relu', solver='adam', max_iter=500)\n","#Bclf = MLPRegressor(hidden_layer_sizes=(100,100), activation='relu', solver='adam', max_iter=500)\n","clf = SVR(kernel='rbf')\n","Bclf = SVR(kernel='rbf')\n","\n","\n","n_runs = 100  # number of repeated CV runs\n","k_folds = 10\n","\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","all_coefs = []\n","Ball_accs, Ball_precisions, Ball_recalls, Ball_f1s, Ball_aucs = [], [], [], [], []\n","Ball_coefs = []\n","\n","random_seed = int(time.time()) # or np.random.randint(0, 10000)\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","\n","\n","    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_seed + run)\n","\n","    accs, precisions, recalls, f1s, aucs, coefs = [], [], [], [], [], []\n","    Baccs, Bprecisions, Brecalls, Bf1s, Baucs, Bcoefs = [], [], [], [], [], []\n","\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","\n","        # ---------------------------\n","        # Mixup (optional)\n","        # ---------------------------\n","#       X_aug, y_aug = intra_mixup(X_train, y_train, n_samples_per_class=40, alpha=0.9) #same class mixup\n","        X_aug, y_aug = inter_mixup(X_train, y_train, n_samples=70, alpha=0.7, hard_labels=False)\n","        X_train_aug = np.vstack([X_train, X_aug])\n","        y_train_aug = np.concatenate([y_train, y_aug])\n","\n","#        print(\"X shape before Mixup:\", X_train.shape)\n","#        print(\"X shape after Mixup:\", X_train_aug.shape)\n","\n","        #X_train_aug, y_train_aug = X_train, y_train  # no mixup\n","        #X_train_aug = add_noise(X_train_aug)\n","\n","        clf.fit(X_train_aug, y_train_aug)\n","        Bclf.fit(X_train, y_train)\n","\n","        y_prob = clf.predict(X[test_idx])\n","#        y_prob = clf.predict_proba(X[test_idx])[:,1]\n","\n","        y_prob = np.clip(y_prob, 0, 1)  # ensure in [0,1]\n","        # Threshold for discrete labels\n","        y_pred = (y_prob >= 0.5).astype(int)\n","\n","\n","        By_prob = Bclf.predict(X[test_idx])\n","#        By_prob = Bclf.predict_proba(X[test_idx])[:,1]\n","        By_prob = np.clip(By_prob, 0, 1)  # ensure in [0,1]\n","        # Threshold for discrete labels\n","        By_pred = (By_prob >= 0.5).astype(int)\n","\n","        #print(np.unique(y_pred, return_counts=True))\n","\n","        accs.append(accuracy_score(y[test_idx], y_pred))\n","        precisions.append(precision_score(y[test_idx], y_pred,zero_division=0))\n","        recalls.append(recall_score(y[test_idx], y_pred,zero_division=0))\n","        f1s.append(f1_score(y[test_idx], y_pred,zero_division=0))\n"," #       aucs.append(roc_auc_score(y[test_idx], y_prob))\n","\n","        Baccs.append(accuracy_score(y[test_idx], By_pred))\n","        Bprecisions.append(precision_score(y[test_idx], By_pred,zero_division=0))\n","        Brecalls.append(recall_score(y[test_idx], By_pred,zero_division=0))\n","        Bf1s.append(f1_score(y[test_idx], By_pred,zero_division=0))\n","#        Baucs.append(roc_auc_score(y[test_idx], By_prob))\n","\n","#        log_reg = clf.named_steps['logisticregression']\n","#        coefs.append(log_reg.coef_[0])\n","\n","#        Blog_reg = Bclf.named_steps['logisticregression']\n","#        Bcoefs.append(log_reg.coef_[0])\n","\n","\n","    # Append this run's results to overall\n","    all_accs.extend(accs)\n","    all_precisions.extend(precisions)\n","    all_recalls.extend(recalls)\n","    all_f1s.extend(f1s)\n","    all_aucs.extend(aucs)\n","  #  all_coefs.append(np.mean(coefs, axis=0))\n","\n","    Ball_accs.extend(Baccs)\n","    Ball_precisions.extend(Bprecisions)\n","    Ball_recalls.extend(Brecalls)\n","    Ball_f1s.extend(Bf1s)\n","    Ball_aucs.extend(Baucs)\n","   # Ball_coefs.append(np.mean(Bcoefs, axis=0))\n","\n","    print(f\"Accuracy Baseline first : {np.mean(Baccs):.2f}\\t{np.mean(accs):.2f}\")\n","# ===========================\n","# 4. Report overall metrics\n","# ===========================\n","print(\"\\n=== Overall : {n_runs}-run {k_folds}-fold CV metrics === Baseline is First Then MixUp\")\n","print(f\"Accuracy  : {np.mean(Ball_accs):.3f}\\t{np.mean(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(Ball_precisions):.3f}\\t{np.mean(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(Ball_recalls):.3f}\\t{np.mean(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(Ball_f1s):.3f}\\t{np.mean(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(Ball_aucs):.3f}\\t{np.mean(all_aucs):.3f}\")\n","\n","# ===========================\n","# 5. Feature importance\n","# ===========================\n","#mean_coef = np.mean(all_coefs, axis=0)\n","#importance = np.abs(mean_coef)\n","\n","#print(\"\\nFeature importance (avg |coef| across runs):\")\n","#for name, imp in zip(feature_names, importance):\n","#    print(f\"{name:<20}: {imp:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rFpZ0G0xwrG","outputId":"23d17ee0-c338-441c-f8e5-33f71c73072a","executionInfo":{"status":"ok","timestamp":1757818895921,"user_tz":420,"elapsed":1523483,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['SID', 'Imp20PercentBPRS', 'Age', 'handedness', 'sex',\n","       'PrimaryEthnicity', 'PrimaryRace', 'Unnamed: 7', 'Education',\n","       'Parental Education', 'Unnamed: 10', 'BPRS_Baseline'],\n","      dtype='object')\n","X shape after NaN removal: (56, 10)\n","feature_flags: {'strength_mean': True, 'strength_std': True, 'clustering_mean': True, 'clustering_std': True, 'betweenness_centrality': True, 'eigenvector_centrality': True, 'avg_clustering': True}\n","29\n","27\n","=== Run 1/100 ===\n","Accuracy Baseline first : 0.68\t0.63\n","=== Run 2/100 ===\n","Accuracy Baseline first : 0.66\t0.63\n","=== Run 3/100 ===\n","Accuracy Baseline first : 0.65\t0.63\n","=== Run 4/100 ===\n","Accuracy Baseline first : 0.69\t0.63\n","=== Run 5/100 ===\n","Accuracy Baseline first : 0.59\t0.61\n","=== Run 6/100 ===\n","Accuracy Baseline first : 0.62\t0.68\n","=== Run 7/100 ===\n","Accuracy Baseline first : 0.71\t0.62\n","=== Run 8/100 ===\n","Accuracy Baseline first : 0.67\t0.69\n","=== Run 9/100 ===\n","Accuracy Baseline first : 0.68\t0.68\n","=== Run 10/100 ===\n","Accuracy Baseline first : 0.68\t0.62\n","=== Run 11/100 ===\n","Accuracy Baseline first : 0.65\t0.56\n","=== Run 12/100 ===\n","Accuracy Baseline first : 0.63\t0.61\n","=== Run 13/100 ===\n","Accuracy Baseline first : 0.64\t0.66\n","=== Run 14/100 ===\n","Accuracy Baseline first : 0.69\t0.62\n","=== Run 15/100 ===\n","Accuracy Baseline first : 0.64\t0.69\n","=== Run 16/100 ===\n","Accuracy Baseline first : 0.63\t0.59\n","=== Run 17/100 ===\n","Accuracy Baseline first : 0.69\t0.73\n","=== Run 18/100 ===\n","Accuracy Baseline first : 0.68\t0.66\n","=== Run 19/100 ===\n","Accuracy Baseline first : 0.66\t0.64\n","=== Run 20/100 ===\n","Accuracy Baseline first : 0.63\t0.72\n","=== Run 21/100 ===\n","Accuracy Baseline first : 0.63\t0.58\n","=== Run 22/100 ===\n","Accuracy Baseline first : 0.68\t0.64\n","=== Run 23/100 ===\n","Accuracy Baseline first : 0.70\t0.66\n","=== Run 24/100 ===\n","Accuracy Baseline first : 0.69\t0.69\n","=== Run 25/100 ===\n","Accuracy Baseline first : 0.63\t0.65\n","=== Run 26/100 ===\n","Accuracy Baseline first : 0.63\t0.59\n","=== Run 27/100 ===\n","Accuracy Baseline first : 0.70\t0.65\n","=== Run 28/100 ===\n","Accuracy Baseline first : 0.65\t0.70\n","=== Run 29/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 30/100 ===\n","Accuracy Baseline first : 0.68\t0.68\n","=== Run 31/100 ===\n","Accuracy Baseline first : 0.69\t0.68\n","=== Run 32/100 ===\n","Accuracy Baseline first : 0.66\t0.70\n","=== Run 33/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 34/100 ===\n","Accuracy Baseline first : 0.72\t0.67\n","=== Run 35/100 ===\n","Accuracy Baseline first : 0.66\t0.72\n","=== Run 36/100 ===\n","Accuracy Baseline first : 0.66\t0.63\n","=== Run 37/100 ===\n","Accuracy Baseline first : 0.67\t0.68\n","=== Run 38/100 ===\n","Accuracy Baseline first : 0.68\t0.63\n","=== Run 39/100 ===\n","Accuracy Baseline first : 0.60\t0.64\n","=== Run 40/100 ===\n","Accuracy Baseline first : 0.64\t0.67\n","=== Run 41/100 ===\n","Accuracy Baseline first : 0.63\t0.61\n","=== Run 42/100 ===\n","Accuracy Baseline first : 0.68\t0.68\n","=== Run 43/100 ===\n","Accuracy Baseline first : 0.66\t0.69\n","=== Run 44/100 ===\n","Accuracy Baseline first : 0.66\t0.63\n","=== Run 45/100 ===\n","Accuracy Baseline first : 0.72\t0.68\n","=== Run 46/100 ===\n","Accuracy Baseline first : 0.64\t0.70\n","=== Run 47/100 ===\n","Accuracy Baseline first : 0.65\t0.68\n","=== Run 48/100 ===\n","Accuracy Baseline first : 0.72\t0.64\n","=== Run 49/100 ===\n","Accuracy Baseline first : 0.65\t0.59\n","=== Run 50/100 ===\n","Accuracy Baseline first : 0.69\t0.65\n","=== Run 51/100 ===\n","Accuracy Baseline first : 0.65\t0.64\n","=== Run 52/100 ===\n","Accuracy Baseline first : 0.63\t0.68\n","=== Run 53/100 ===\n","Accuracy Baseline first : 0.57\t0.60\n","=== Run 54/100 ===\n","Accuracy Baseline first : 0.72\t0.70\n","=== Run 55/100 ===\n","Accuracy Baseline first : 0.65\t0.64\n","=== Run 56/100 ===\n","Accuracy Baseline first : 0.71\t0.66\n","=== Run 57/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 58/100 ===\n","Accuracy Baseline first : 0.67\t0.71\n","=== Run 59/100 ===\n","Accuracy Baseline first : 0.64\t0.64\n","=== Run 60/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 61/100 ===\n","Accuracy Baseline first : 0.71\t0.65\n","=== Run 62/100 ===\n","Accuracy Baseline first : 0.64\t0.64\n","=== Run 63/100 ===\n","Accuracy Baseline first : 0.66\t0.62\n","=== Run 64/100 ===\n","Accuracy Baseline first : 0.69\t0.66\n","=== Run 65/100 ===\n","Accuracy Baseline first : 0.68\t0.74\n","=== Run 66/100 ===\n","Accuracy Baseline first : 0.70\t0.70\n","=== Run 67/100 ===\n","Accuracy Baseline first : 0.70\t0.70\n","=== Run 68/100 ===\n","Accuracy Baseline first : 0.73\t0.66\n","=== Run 69/100 ===\n","Accuracy Baseline first : 0.69\t0.64\n","=== Run 70/100 ===\n","Accuracy Baseline first : 0.69\t0.67\n","=== Run 71/100 ===\n","Accuracy Baseline first : 0.61\t0.64\n","=== Run 72/100 ===\n","Accuracy Baseline first : 0.65\t0.67\n","=== Run 73/100 ===\n","Accuracy Baseline first : 0.68\t0.71\n","=== Run 74/100 ===\n","Accuracy Baseline first : 0.66\t0.66\n","=== Run 75/100 ===\n","Accuracy Baseline first : 0.63\t0.69\n","=== Run 76/100 ===\n","Accuracy Baseline first : 0.68\t0.66\n","=== Run 77/100 ===\n","Accuracy Baseline first : 0.66\t0.69\n","=== Run 78/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 79/100 ===\n","Accuracy Baseline first : 0.65\t0.65\n","=== Run 80/100 ===\n","Accuracy Baseline first : 0.66\t0.60\n","=== Run 81/100 ===\n","Accuracy Baseline first : 0.71\t0.66\n","=== Run 82/100 ===\n","Accuracy Baseline first : 0.68\t0.65\n","=== Run 83/100 ===\n","Accuracy Baseline first : 0.71\t0.70\n","=== Run 84/100 ===\n","Accuracy Baseline first : 0.68\t0.63\n","=== Run 85/100 ===\n","Accuracy Baseline first : 0.66\t0.59\n","=== Run 86/100 ===\n","Accuracy Baseline first : 0.63\t0.58\n","=== Run 87/100 ===\n","Accuracy Baseline first : 0.64\t0.55\n","=== Run 88/100 ===\n","Accuracy Baseline first : 0.61\t0.60\n","=== Run 89/100 ===\n","Accuracy Baseline first : 0.67\t0.65\n","=== Run 90/100 ===\n","Accuracy Baseline first : 0.63\t0.62\n","=== Run 91/100 ===\n","Accuracy Baseline first : 0.64\t0.60\n","=== Run 92/100 ===\n","Accuracy Baseline first : 0.64\t0.66\n","=== Run 93/100 ===\n","Accuracy Baseline first : 0.65\t0.58\n","=== Run 94/100 ===\n","Accuracy Baseline first : 0.63\t0.65\n","=== Run 95/100 ===\n","Accuracy Baseline first : 0.66\t0.68\n","=== Run 96/100 ===\n","Accuracy Baseline first : 0.60\t0.61\n","=== Run 97/100 ===\n","Accuracy Baseline first : 0.66\t0.64\n","=== Run 98/100 ===\n","Accuracy Baseline first : 0.68\t0.62\n","=== Run 99/100 ===\n","Accuracy Baseline first : 0.63\t0.63\n","=== Run 100/100 ===\n","Accuracy Baseline first : 0.64\t0.70\n","\n","=== Overall : {n_runs}-run {k_folds}-fold CV metrics === Baseline is First Then MixUp\n","Accuracy  : 0.662\t0.651\n","Precision : 0.697\t0.691\n","Recall    : 0.630\t0.624\n","F1-score  : 0.631\t0.621\n","AUC       : nan\tnan\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]}]},{"cell_type":"code","source":["#clf = make_pipeline(StandardScaler(), SGDRegressor(max_iter=500, tol=1e-3))\n","#Bclf = make_pipeline(StandardScaler(), SGDRegressor(max_iter=500, tol=1e-3))\n","clf = MLPRegressor(hidden_layer_sizes=(100,100), activation='relu', solver='adam', max_iter=500)\n","Bclf = MLPRegressor(hidden_layer_sizes=(100,100), activation='relu', solver='adam', max_iter=500)\n","#clf = SVR(kernel='rbf')\n","#Bclf = SVR(kernel='rbf')\n","\n","\n","n_runs = 100  # number of repeated CV runs\n","k_folds = 10\n","\n","all_accs, all_precisions, all_recalls, all_f1s, all_aucs = [], [], [], [], []\n","all_coefs = []\n","Ball_accs, Ball_precisions, Ball_recalls, Ball_f1s, Ball_aucs = [], [], [], [], []\n","Ball_coefs = []\n","\n","random_seed = int(time.time()) # or np.random.randint(0, 10000)\n","for run in range(n_runs):\n","    print(f\"=== Run {run+1}/{n_runs} ===\")\n","\n","\n","    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_seed + run)\n","\n","    accs, precisions, recalls, f1s, aucs, coefs = [], [], [], [], [], []\n","    Baccs, Bprecisions, Brecalls, Bf1s, Baucs, Bcoefs = [], [], [], [], [], []\n","\n","\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","\n","        # ---------------------------\n","        # Mixup (optional)\n","        # ---------------------------\n","#       X_aug, y_aug = intra_mixup(X_train, y_train, n_samples_per_class=40, alpha=0.9) #same class mixup\n","        X_aug, y_aug = inter_mixup(X_train, y_train, n_samples=70, alpha=0.7, hard_labels=False)\n","        X_train_aug = np.vstack([X_train, X_aug])\n","        y_train_aug = np.concatenate([y_train, y_aug])\n","\n","#        print(\"X shape before Mixup:\", X_train.shape)\n","#        print(\"X shape after Mixup:\", X_train_aug.shape)\n","\n","        #X_train_aug, y_train_aug = X_train, y_train  # no mixup\n","        #X_train_aug = add_noise(X_train_aug)\n","\n","        clf.fit(X_train_aug, y_train_aug)\n","        Bclf.fit(X_train, y_train)\n","\n","        y_prob = clf.predict(X[test_idx])\n","#        y_prob = clf.predict_proba(X[test_idx])[:,1]\n","\n","        y_prob = np.clip(y_prob, 0, 1)  # ensure in [0,1]\n","        # Threshold for discrete labels\n","        y_pred = (y_prob >= 0.5).astype(int)\n","\n","\n","        By_prob = Bclf.predict(X[test_idx])\n","#        By_prob = Bclf.predict_proba(X[test_idx])[:,1]\n","        By_prob = np.clip(By_prob, 0, 1)  # ensure in [0,1]\n","        # Threshold for discrete labels\n","        By_pred = (By_prob >= 0.5).astype(int)\n","\n","        #print(np.unique(y_pred, return_counts=True))\n","\n","        accs.append(accuracy_score(y[test_idx], y_pred))\n","        precisions.append(precision_score(y[test_idx], y_pred,zero_division=0))\n","        recalls.append(recall_score(y[test_idx], y_pred,zero_division=0))\n","        f1s.append(f1_score(y[test_idx], y_pred,zero_division=0))\n"," #       aucs.append(roc_auc_score(y[test_idx], y_prob))\n","\n","        Baccs.append(accuracy_score(y[test_idx], By_pred))\n","        Bprecisions.append(precision_score(y[test_idx], By_pred,zero_division=0))\n","        Brecalls.append(recall_score(y[test_idx], By_pred,zero_division=0))\n","        Bf1s.append(f1_score(y[test_idx], By_pred,zero_division=0))\n","#        Baucs.append(roc_auc_score(y[test_idx], By_prob))\n","\n","#        log_reg = clf.named_steps['logisticregression']\n","#        coefs.append(log_reg.coef_[0])\n","\n","#        Blog_reg = Bclf.named_steps['logisticregression']\n","#        Bcoefs.append(log_reg.coef_[0])\n","\n","\n","    # Append this run's results to overall\n","    all_accs.extend(accs)\n","    all_precisions.extend(precisions)\n","    all_recalls.extend(recalls)\n","    all_f1s.extend(f1s)\n","    all_aucs.extend(aucs)\n","  #  all_coefs.append(np.mean(coefs, axis=0))\n","\n","    Ball_accs.extend(Baccs)\n","    Ball_precisions.extend(Bprecisions)\n","    Ball_recalls.extend(Brecalls)\n","    Ball_f1s.extend(Bf1s)\n","    Ball_aucs.extend(Baucs)\n","   # Ball_coefs.append(np.mean(Bcoefs, axis=0))\n","\n","    print(f\"Accuracy Baseline first : {np.mean(Baccs):.2f}\\t{np.mean(accs):.2f}\")\n","# ===========================\n","# 4. Report overall metrics\n","# ===========================\n","print(\"\\n=== Overall : {n_runs}-run {k_folds}-fold CV metrics === Baseline is First Then MixUp\")\n","print(f\"Accuracy  : {np.mean(Ball_accs):.3f}\\t{np.mean(all_accs):.3f}\")\n","print(f\"Precision : {np.mean(Ball_precisions):.3f}\\t{np.mean(all_precisions):.3f}\")\n","print(f\"Recall    : {np.mean(Ball_recalls):.3f}\\t{np.mean(all_recalls):.3f}\")\n","print(f\"F1-score  : {np.mean(Ball_f1s):.3f}\\t{np.mean(all_f1s):.3f}\")\n","print(f\"AUC       : {np.mean(Ball_aucs):.3f}\\t{np.mean(all_aucs):.3f}\")\n","\n","# ===========================\n","# 5. Feature importance\n","# ===========================\n","#mean_coef = np.mean(all_coefs, axis=0)\n","#importance = np.abs(mean_coef)\n","\n","#print(\"\\nFeature importance (avg |coef| across runs):\")\n","#for name, imp in zip(feature_names, importance):\n","#    print(f\"{name:<20}: {imp:.3f}\")"],"metadata":{"id":"RtDYhDN2uZ01","executionInfo":{"status":"ok","timestamp":1757819457788,"user_tz":420,"elapsed":370756,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bc7fc2b-f93c-42c3-825c-00d32c301fae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Run 1/100 ===\n","Accuracy Baseline first : 0.65\t0.66\n","=== Run 2/100 ===\n","Accuracy Baseline first : 0.70\t0.57\n","=== Run 3/100 ===\n","Accuracy Baseline first : 0.63\t0.63\n","=== Run 4/100 ===\n","Accuracy Baseline first : 0.62\t0.66\n","=== Run 5/100 ===\n","Accuracy Baseline first : 0.59\t0.59\n","=== Run 6/100 ===\n","Accuracy Baseline first : 0.65\t0.65\n","=== Run 7/100 ===\n","Accuracy Baseline first : 0.66\t0.63\n","=== Run 8/100 ===\n","Accuracy Baseline first : 0.62\t0.56\n","=== Run 9/100 ===\n","Accuracy Baseline first : 0.63\t0.67\n","=== Run 10/100 ===\n","Accuracy Baseline first : 0.71\t0.58\n","=== Run 11/100 ===\n","Accuracy Baseline first : 0.68\t0.64\n","=== Run 12/100 ===\n","Accuracy Baseline first : 0.66\t0.64\n","=== Run 13/100 ===\n","Accuracy Baseline first : 0.60\t0.63\n","=== Run 14/100 ===\n","Accuracy Baseline first : 0.70\t0.56\n","=== Run 15/100 ===\n","Accuracy Baseline first : 0.64\t0.59\n","=== Run 16/100 ===\n","Accuracy Baseline first : 0.53\t0.59\n","=== Run 17/100 ===\n","Accuracy Baseline first : 0.64\t0.67\n","=== Run 18/100 ===\n","Accuracy Baseline first : 0.64\t0.61\n","=== Run 19/100 ===\n","Accuracy Baseline first : 0.60\t0.65\n","=== Run 20/100 ===\n","Accuracy Baseline first : 0.66\t0.64\n","=== Run 21/100 ===\n","Accuracy Baseline first : 0.63\t0.66\n","=== Run 22/100 ===\n","Accuracy Baseline first : 0.72\t0.70\n","=== Run 23/100 ===\n","Accuracy Baseline first : 0.68\t0.54\n","=== Run 24/100 ===\n","Accuracy Baseline first : 0.57\t0.59\n","=== Run 25/100 ===\n","Accuracy Baseline first : 0.46\t0.66\n","=== Run 26/100 ===\n","Accuracy Baseline first : 0.67\t0.60\n","=== Run 27/100 ===\n","Accuracy Baseline first : 0.69\t0.68\n","=== Run 28/100 ===\n","Accuracy Baseline first : 0.61\t0.63\n","=== Run 29/100 ===\n","Accuracy Baseline first : 0.61\t0.56\n","=== Run 30/100 ===\n","Accuracy Baseline first : 0.65\t0.63\n","=== Run 31/100 ===\n","Accuracy Baseline first : 0.69\t0.69\n","=== Run 32/100 ===\n","Accuracy Baseline first : 0.68\t0.63\n","=== Run 33/100 ===\n","Accuracy Baseline first : 0.66\t0.55\n","=== Run 34/100 ===\n","Accuracy Baseline first : 0.65\t0.71\n","=== Run 35/100 ===\n","Accuracy Baseline first : 0.66\t0.65\n","=== Run 36/100 ===\n","Accuracy Baseline first : 0.66\t0.64\n","=== Run 37/100 ===\n","Accuracy Baseline first : 0.57\t0.72\n","=== Run 38/100 ===\n","Accuracy Baseline first : 0.56\t0.66\n","=== Run 39/100 ===\n","Accuracy Baseline first : 0.64\t0.61\n","=== Run 40/100 ===\n","Accuracy Baseline first : 0.64\t0.61\n","=== Run 41/100 ===\n","Accuracy Baseline first : 0.53\t0.62\n","=== Run 42/100 ===\n","Accuracy Baseline first : 0.71\t0.66\n","=== Run 43/100 ===\n","Accuracy Baseline first : 0.62\t0.57\n","=== Run 44/100 ===\n","Accuracy Baseline first : 0.65\t0.60\n","=== Run 45/100 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy Baseline first : 0.63\t0.68\n","=== Run 46/100 ===\n","Accuracy Baseline first : 0.77\t0.65\n","=== Run 47/100 ===\n","Accuracy Baseline first : 0.65\t0.63\n","=== Run 48/100 ===\n","Accuracy Baseline first : 0.56\t0.55\n","=== Run 49/100 ===\n","Accuracy Baseline first : 0.73\t0.71\n","=== Run 50/100 ===\n","Accuracy Baseline first : 0.63\t0.57\n","=== Run 51/100 ===\n","Accuracy Baseline first : 0.71\t0.63\n","=== Run 52/100 ===\n","Accuracy Baseline first : 0.63\t0.57\n","=== Run 53/100 ===\n","Accuracy Baseline first : 0.69\t0.65\n","=== Run 54/100 ===\n","Accuracy Baseline first : 0.65\t0.59\n","=== Run 55/100 ===\n","Accuracy Baseline first : 0.68\t0.60\n","=== Run 56/100 ===\n","Accuracy Baseline first : 0.64\t0.66\n","=== Run 57/100 ===\n","Accuracy Baseline first : 0.71\t0.61\n","=== Run 58/100 ===\n","Accuracy Baseline first : 0.63\t0.60\n","=== Run 59/100 ===\n","Accuracy Baseline first : 0.64\t0.61\n","=== Run 60/100 ===\n","Accuracy Baseline first : 0.64\t0.72\n","=== Run 61/100 ===\n","Accuracy Baseline first : 0.68\t0.50\n","=== Run 62/100 ===\n","Accuracy Baseline first : 0.61\t0.52\n","=== Run 63/100 ===\n","Accuracy Baseline first : 0.64\t0.59\n","=== Run 64/100 ===\n","Accuracy Baseline first : 0.59\t0.68\n","=== Run 65/100 ===\n","Accuracy Baseline first : 0.62\t0.66\n","=== Run 66/100 ===\n","Accuracy Baseline first : 0.63\t0.65\n","=== Run 67/100 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy Baseline first : 0.59\t0.64\n","=== Run 68/100 ===\n","Accuracy Baseline first : 0.61\t0.75\n","=== Run 69/100 ===\n","Accuracy Baseline first : 0.63\t0.72\n","=== Run 70/100 ===\n","Accuracy Baseline first : 0.66\t0.66\n","=== Run 71/100 ===\n","Accuracy Baseline first : 0.72\t0.69\n","=== Run 72/100 ===\n","Accuracy Baseline first : 0.65\t0.70\n","=== Run 73/100 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy Baseline first : 0.60\t0.65\n","=== Run 74/100 ===\n","Accuracy Baseline first : 0.64\t0.60\n","=== Run 75/100 ===\n","Accuracy Baseline first : 0.66\t0.63\n","=== Run 76/100 ===\n","Accuracy Baseline first : 0.59\t0.59\n","=== Run 77/100 ===\n","Accuracy Baseline first : 0.69\t0.57\n","=== Run 78/100 ===\n","Accuracy Baseline first : 0.61\t0.64\n","=== Run 79/100 ===\n","Accuracy Baseline first : 0.73\t0.67\n","=== Run 80/100 ===\n","Accuracy Baseline first : 0.59\t0.63\n","=== Run 81/100 ===\n","Accuracy Baseline first : 0.57\t0.73\n","=== Run 82/100 ===\n","Accuracy Baseline first : 0.61\t0.63\n","=== Run 83/100 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy Baseline first : 0.61\t0.62\n","=== Run 84/100 ===\n","Accuracy Baseline first : 0.75\t0.62\n","=== Run 85/100 ===\n","Accuracy Baseline first : 0.62\t0.67\n","=== Run 86/100 ===\n","Accuracy Baseline first : 0.70\t0.66\n","=== Run 87/100 ===\n","Accuracy Baseline first : 0.64\t0.75\n","=== Run 88/100 ===\n","Accuracy Baseline first : 0.66\t0.62\n","=== Run 89/100 ===\n","Accuracy Baseline first : 0.62\t0.68\n","=== Run 90/100 ===\n","Accuracy Baseline first : 0.70\t0.58\n","=== Run 91/100 ===\n","Accuracy Baseline first : 0.70\t0.64\n","=== Run 92/100 ===\n","Accuracy Baseline first : 0.63\t0.64\n","=== Run 93/100 ===\n","Accuracy Baseline first : 0.61\t0.70\n","=== Run 94/100 ===\n","Accuracy Baseline first : 0.64\t0.70\n","=== Run 95/100 ===\n","Accuracy Baseline first : 0.66\t0.67\n","=== Run 96/100 ===\n","Accuracy Baseline first : 0.61\t0.67\n","=== Run 97/100 ===\n","Accuracy Baseline first : 0.60\t0.65\n","=== Run 98/100 ===\n","Accuracy Baseline first : 0.56\t0.65\n","=== Run 99/100 ===\n","Accuracy Baseline first : 0.63\t0.55\n","=== Run 100/100 ===\n","Accuracy Baseline first : 0.70\t0.56\n","\n","=== Overall : {n_runs}-run {k_folds}-fold CV metrics === Baseline is First Then MixUp\n","Accuracy  : 0.641\t0.633\n","Precision : 0.596\t0.583\n","Recall    : 0.677\t0.642\n","F1-score  : 0.603\t0.577\n","AUC       : nan\tnan\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"l4HPVkrq32jm"},"execution_count":null,"outputs":[]}]}